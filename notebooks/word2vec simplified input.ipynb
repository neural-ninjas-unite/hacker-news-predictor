{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import more_itertools\n",
    "import random\n",
    "from torchmetrics import Accuracy\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Tokenisation-related libraries\n",
    "import re\n",
    "from typing import Union, List\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "\n",
    "# Setup device agnostic code\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "device\n",
    "\n",
    "# Setup random seed\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download all necessary NLTK data\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('punkt_tab')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 100 characters:\n",
      " anarchism originated as a term of abuse first used against early working class radicals including the diggers of the english revolution and the sans culottes of the french revolution whilst the term is still used in a pejorative way to describe any act that used violent means to destroy the organization of society it has also been taken up as a positive label by self defined anarchists the word anarchism is derived from the greek without archons ruler chief king anarchism as a political philosophy is the belief that rulers are unnecessary and should be abolished although there are differing interpretations of what this means anarchism also refers to related social movements that advocate the elimination of authoritarian institutions particularly the state the word anarchy as most anarchists use it does not imply chaos nihilism or anomie but rather a harmonious anti authoritarian society in place of what are regarded as authoritarian political structures and coercive economic instituti\n",
      "\n",
      "Total characters: 100000000\n",
      "\n",
      "Total words (raw split): 17005207\n",
      "Initial vocabulary size: 253854\n"
     ]
    }
   ],
   "source": [
    "# Read the text8 file\n",
    "with open('../scripts/text8', 'r', encoding='utf-8') as file:\n",
    "    text8_data = file.read()\n",
    "\n",
    "# Print first few characters to verify\n",
    "print(\"First 100 characters:\")\n",
    "print(text8_data[:1000])\n",
    "\n",
    "# Print total length\n",
    "print(f\"\\nTotal characters: {len(text8_data)}\")\n",
    "\n",
    "# Get initial word count (simple split)\n",
    "initial_words = text8_data.split()\n",
    "characters = len(text8_data) / 100\n",
    "print(f\"\\nTotal words (raw split): {len(initial_words)}\")\n",
    "print(f\"Initial vocabulary size: {len(set(initial_words))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.0</td>\n",
       "      <td>nasa's 3d-printed rotating detonation rocket e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>62.0</td>\n",
       "      <td>heat pumps of the 1800s are becoming the techn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>why you should develop local-first web apps</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>tool to make twitter archive publishable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>fedora packages versus upstream flatpaks</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   score                                              title\n",
       "0    8.0  nasa's 3d-printed rotating detonation rocket e...\n",
       "1   62.0  heat pumps of the 1800s are becoming the techn...\n",
       "2    1.0        why you should develop local-first web apps\n",
       "3    1.0           tool to make twitter archive publishable\n",
       "4    2.0           fedora packages versus upstream flatpaks"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../HN Score, Title 10k.csv')\n",
    "df.title = df.title.str.lower()\n",
    "df.dropna(inplace= True) # Remove rows with incomplete data\n",
    "\n",
    "# Convert titles to a single string\n",
    "titles_text = ' '.join(df.title.astype(str).tolist())\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words = []\n",
    "# num_lines = 10000\n",
    "# lines = df.title.tolist()[:num_lines]\n",
    "\n",
    "# for i in df.title[:num_lines]:\n",
    "#     for j in str(i).split():\n",
    "#         if j not in words and j != \"nan\":\n",
    "#             words.append(j)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "# itos = {num:word for num, word in zip(range(len(words)),words)}\n",
    "# stoi = {word:num for num,word in itos.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text_input: Union[str, List[str]], min_freq: int = 5) -> tuple[List[str], dict]:\n",
    "    \"\"\"\n",
    "    Preprocess text input and create vocabulary.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialise Lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # Define special tokens using a marker that word_tokenize won't split\n",
    "    SPECIAL_TOKENS = {\n",
    "        '.': 'XPERIODX',\n",
    "        ',': 'XCOMMAX',\n",
    "        '\"': 'XQUOTATION_MARKX',\n",
    "        ';': 'XSEMICOLONX',\n",
    "        '!': 'XEXCLAMATION_MARKX',\n",
    "        '?': 'XQUESTION_MARKX',\n",
    "        '(': 'XLEFT_PARENX',\n",
    "        ')': 'XRIGHT_PARENX',\n",
    "        '--': 'XHYPHENSX',\n",
    "        ':': 'XCOLONX',\n",
    "        \"'\": 'XAPOSTROPHEX'\n",
    "    }\n",
    "    \n",
    "    # Mapping for restoring angle brackets\n",
    "    RESTORE_TOKENS = {\n",
    "        f'XPERIODX': '<PERIOD>',\n",
    "        f'XCOMMAX': '<COMMA>',\n",
    "        f'XQUOTATION_MARKX': '<QUOTATION_MARK>',\n",
    "        f'XSEMICOLONX': '<SEMICOLON>',\n",
    "        f'XEXCLAMATION_MARKX': '<EXCLAMATION_MARK>',\n",
    "        f'XQUESTION_MARKX': '<QUESTION_MARK>',\n",
    "        f'XLEFT_PARENX': '<LEFT_PAREN>',\n",
    "        f'XRIGHT_PARENX': '<RIGHT_PAREN>',\n",
    "        f'XHYPHENSX': '<HYPHENS>',\n",
    "        f'XCOLONX': '<COLON>',\n",
    "        f'XAPOSTROPHEX': '<APOSTROPHE>'\n",
    "    }\n",
    "    \n",
    "    def clean_text(text: str) -> str:\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Replace special characters with temporary tokens\n",
    "        for char, token in SPECIAL_TOKENS.items():\n",
    "            text = text.replace(char, f' {token} ')\n",
    "        \n",
    "        # Remove special characters and extra whitespace\n",
    "        text = re.sub(r'[^a-zA-Z0-9\\s_X]', '', text)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        \n",
    "        return text.strip()\n",
    "\n",
    "    # Convert input to list of sentences\n",
    "    if isinstance(text_input, str):\n",
    "        sentences = sent_tokenize(text_input)\n",
    "    elif isinstance(text_input, list):\n",
    "        sentences = []\n",
    "        for line in text_input:\n",
    "            if isinstance(line, str):\n",
    "                try:\n",
    "                    sentences.extend(sent_tokenize(line))\n",
    "                except:\n",
    "                    sentences.append(line)\n",
    "    else:\n",
    "        raise ValueError(\"Input must be either a string or list of strings\")\n",
    "\n",
    "    # Process all sentences\n",
    "    processed_words = []\n",
    "    for sentence in sentences:\n",
    "        # Clean the text\n",
    "        cleaned_text = clean_text(sentence)\n",
    "        # Tokenize\n",
    "        try:\n",
    "            words = word_tokenize(cleaned_text)\n",
    "            # Restore angle bracket format\n",
    "            words = [RESTORE_TOKENS.get(word, word) for word in words]\n",
    "            # Apply lemmatization only to non-special tokens\n",
    "            words = [word if word in RESTORE_TOKENS.values() \n",
    "                    else lemmatizer.lemmatize(word) \n",
    "                    for word in words]\n",
    "        except:\n",
    "            words = cleaned_text.split()\n",
    "        processed_words.extend(words)\n",
    "\n",
    "    # Count word frequencies\n",
    "    word_counts = Counter(processed_words)\n",
    "    \n",
    "    # Create vocabulary (only including words that meet minimum frequency)\n",
    "    vocab = [word for word, count in word_counts.items() if count >= min_freq]\n",
    "    word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "    \n",
    "    # Filter words based on vocabulary\n",
    "    processed_words = [word for word in processed_words if word in word_to_idx]\n",
    "    \n",
    "    return processed_words, word_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Process text inputs, receive tokens and word to index dictionary\n",
    "# processed_words, word_to_idx = preprocess_text(text8_data, min_freq=5)\n",
    "\n",
    "# # Set vocab_size, determine dimensions of layers in model\n",
    "# vocab_size = len(word_to_idx)\n",
    "\n",
    "# len(processed_words)\n",
    "# len(word_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update 1\n",
    "\n",
    "def create_skipgram_pairs(processed_words: List[str], \n",
    "                         word_to_idx: dict, \n",
    "                         context_len: int = 3) -> tuple[List[int], List[List[int]]]:\n",
    "    \"\"\"\n",
    "    Create skipgram pairs from a list of processed words.\n",
    "    \"\"\"\n",
    "    if context_len % 2 == 0:\n",
    "        raise ValueError(\"context_len should be an odd number\")\n",
    "        \n",
    "    window_radius = context_len // 2\n",
    "    input_indices = []\n",
    "    context_indices = []\n",
    "    vocab_size = len(word_to_idx)  # Added this line\n",
    "    \n",
    "    # Use sliding window to create pairs\n",
    "    windows = list(more_itertools.windowed(processed_words, context_len))\n",
    "    \n",
    "    for window in windows:\n",
    "        if None in window:  # Skip incomplete windows\n",
    "            continue\n",
    "            \n",
    "        # Get center word and context\n",
    "        center_word = window[window_radius]\n",
    "        context = list(window[:window_radius]) + list(window[window_radius + 1:])\n",
    "        \n",
    "        # Only add if all words are in vocabulary AND indices are within range\n",
    "        if center_word in word_to_idx and all(w in word_to_idx for w in context):\n",
    "            center_idx = word_to_idx[center_word]\n",
    "            context_idxs = [word_to_idx[w] for w in context]\n",
    "            \n",
    "            # Added this check\n",
    "            if center_idx < vocab_size and all(idx < vocab_size for idx in context_idxs):\n",
    "                input_indices.append(center_idx)\n",
    "                context_indices.append(context_idxs)\n",
    "    \n",
    "    return input_indices, context_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_skipgram_pairs(processed_words: List[str], \n",
    "#                          word_to_idx: dict, \n",
    "#                          context_len: int = 3) -> tuple[List[int], List[List[int]]]:\n",
    "#     \"\"\"\n",
    "#     Create skipgram pairs from a list of processed words.\n",
    "#     \"\"\"\n",
    "#     if context_len % 2 == 0:\n",
    "#         raise ValueError(\"context_len should be an odd number\")\n",
    "        \n",
    "#     window_radius = context_len // 2\n",
    "#     input_indices = []\n",
    "#     context_indices = []\n",
    "    \n",
    "#     # Use sliding window to create pairs\n",
    "#     windows = list(more_itertools.windowed(processed_words, context_len))\n",
    "    \n",
    "#     for window in windows:\n",
    "#         if None in window:  # Skip incomplete windows\n",
    "#             continue\n",
    "            \n",
    "#         # Get center word and context\n",
    "#         center_word = window[window_radius]\n",
    "#         context = list(window[:window_radius]) + list(window[window_radius + 1:])\n",
    "        \n",
    "#         # Only add if all words are in vocabulary\n",
    "#         if center_word in word_to_idx and all(w in word_to_idx for w in context):\n",
    "#             input_indices.append(word_to_idx[center_word])\n",
    "#             context_indices.append([word_to_idx[w] for w in context])\n",
    "    \n",
    "#     return input_indices, context_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 3727\n",
      "Original text vocabulary size: 17110\n",
      "Dataset size: 149736\n",
      "Sample from vocabulary: ['anarchism', 'originated', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against', 'early']\n",
      "Sample from processed words: ['anarchism', 'originated', 'a', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against']\n",
      "X shape:  torch.Size([149736]) ; Y shape:  torch.Size([149736, 2]) ; Shapes compatible:  True\n"
     ]
    }
   ],
   "source": [
    "# Concatenate the Wikipedia & Hacker News datasets\n",
    "combined_text = text8_data + \" \" + titles_text\n",
    "\n",
    "# Determine input size\n",
    "input=text8_data[:int(len(combined_text)/100)]\n",
    "\n",
    "# Process text inputs, receive tokens and word to index dictionary\n",
    "processed_words, word_to_idx = preprocess_text(input, min_freq=5)\n",
    "\n",
    "# Set vocab_size, determine dimensions of layers in model\n",
    "vocab_size = len(word_to_idx)\n",
    "\n",
    "# Set context length\n",
    "context_len = 3\n",
    "\n",
    "# Create Skip-gram dataset using processed, tokenized text input:\n",
    "input_indices, context_indices = create_skipgram_pairs(processed_words, word_to_idx, context_len)\n",
    "\n",
    "# Convert dataset indices to tensors\n",
    "X = torch.tensor(input_indices, dtype=torch.long)\n",
    "Y = torch.tensor(context_indices, dtype=torch.long)\n",
    "\n",
    "# Debug prints\n",
    "print(\"Vocabulary size:\", len(word_to_idx))\n",
    "print(\"Original text vocabulary size:\", len(words))\n",
    "print(\"Dataset size:\", len(input_indices))\n",
    "print(\"Sample from vocabulary:\", list(word_to_idx.keys())[:10])\n",
    "print(\"Sample from processed words:\", processed_words[:10])\n",
    "print(\"X shape: \", X.shape, \"; Y shape: \", Y.shape, \"; Shapes compatible: \", X.shape[0] == Y.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample from vocabulary: ['anarchism', 'originated', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against', 'early']\n"
     ]
    }
   ],
   "source": [
    "print(\"Sample from vocabulary:\", list(word_to_idx)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1, 2, 2, 3, 4, 5, 6, 7, 8, 9]),\n",
       " tensor([[ 0,  2],\n",
       "         [ 1,  2],\n",
       "         [ 2,  3],\n",
       "         [ 2,  4],\n",
       "         [ 3,  5],\n",
       "         [ 4,  6],\n",
       "         [ 5,  7],\n",
       "         [ 6,  8],\n",
       "         [ 7,  9],\n",
       "         [ 8, 10]]))"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Samples of tensors\n",
    "X[:10], Y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 16735778 ; context length: 3 ; vocab size: 63774\n"
     ]
    }
   ],
   "source": [
    "inputs_len = len(X)\n",
    "print(\"Dataset size:\", inputs_len, \"; context length:\", context_len, \"; vocab size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set embedding dimensions\n",
    "emb_dims = 20\n",
    "\n",
    "# Build Word2Vec model\n",
    "class Word2Vec(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dims):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embeddings = nn.Embedding(num_embeddings= vocab_size, embedding_dim= emb_dims)\n",
    "        self.output_weights = nn.Linear(in_features = emb_dims, out_features = vocab_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x, positive_samples, negative_samples):\n",
    "        # Add dimension checks\n",
    "        if x.max() >= self.vocab_size or positive_samples.max() >= self.vocab_size or negative_samples.max() >= self.vocab_size:\n",
    "            raise ValueError(f\"Input indices must be less than vocab_size ({self.vocab_size})\")\n",
    "\n",
    "        emb = self.embeddings(x)\n",
    "        context_weights = self.output_weights.weight[positive_samples]\n",
    "        negative_sample_weights = self.output_weights.weight[negative_samples]\n",
    "        positive_out = torch.bmm(context_weights, emb.unsqueeze(-1)).squeeze(-1)\n",
    "        negative_out = torch.bmm(negative_sample_weights, emb.unsqueeze(-1)).squeeze(-1)\n",
    "        positive_out = self.sigmoid(positive_out)\n",
    "        negative_out = self.sigmoid(negative_out)\n",
    "        positive_loss = -positive_out.log().mean()\n",
    "        negative_loss = -(1 - negative_out + 10**(-3)).log().mean()\n",
    "        return positive_loss + negative_loss\n",
    "\n",
    "# Instantiate model\n",
    "word2vec = Word2Vec(vocab_size = vocab_size, emb_dims= emb_dims)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup loss function\n",
    "# loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Set up optimizer to optimize model's parameters\n",
    "optimiser = torch.optim.SGD(params= word2vec.parameters(), lr = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(model, X, Y, vocab_size):\n",
    "    \"\"\"\n",
    "    Calculate accuracy by comparing positive sample scores with negative sample scores.\n",
    "    We want positive context words to have higher scores than random negative words.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Get embeddings for all input words\n",
    "        emb = model.embeddings(X)  # [num_samples, embedding_dim]\n",
    "        \n",
    "        # Get positive context embeddings\n",
    "        pos_ctx = model.output_weights.weight[Y]  # [num_samples, 2, embedding_dim]\n",
    "        \n",
    "        # Generate negative samples\n",
    "        neg_samples = torch.randint(0, vocab_size, Y.shape)\n",
    "        neg_ctx = model.output_weights.weight[neg_samples]\n",
    "        \n",
    "        # Calculate similarity scores\n",
    "        emb_reshaped = emb.unsqueeze(-1)  # [num_samples, embedding_dim, 1]\n",
    "        pos_scores = torch.bmm(pos_ctx, emb_reshaped).squeeze(-1)  # [num_samples, 2]\n",
    "        neg_scores = torch.bmm(neg_ctx, emb_reshaped).squeeze(-1)  # [num_samples, 2]\n",
    "        \n",
    "        # Accuracy: how often positive scores > negative scores\n",
    "        accuracy = (pos_scores > neg_scores).float().mean().item()\n",
    "        \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check where input len is used, delete if otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 16735778\n",
      "Vocabulary size: 63774\n",
      "Max index in X: 63773\n",
      "Max index in Y: 63773\n",
      "Sample X values: tensor([1, 2, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "Sample Y values: tensor([[ 0,  2],\n",
      "        [ 1,  2],\n",
      "        [ 2,  3],\n",
      "        [ 2,  4],\n",
      "        [ 3,  5],\n",
      "        [ 4,  6],\n",
      "        [ 5,  7],\n",
      "        [ 6,  8],\n",
      "        [ 7,  9],\n",
      "        [ 8, 10]])\n"
     ]
    }
   ],
   "source": [
    "# After creating the dataset\n",
    "print(\"Dataset size:\", len(X))\n",
    "print(\"Vocabulary size:\", len(word_to_idx))\n",
    "print(\"Max index in X:\", X.max().item())\n",
    "print(\"Max index in Y:\", Y.max().item())\n",
    "print(\"Sample X values:\", X[:10])\n",
    "print(\"Sample Y values:\", Y[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shapes - X: torch.Size([32]), Pos: torch.Size([32, 2]), Neg: torch.Size([32, 2])\n",
      "Max indices - X: 3717, Pos: 3395, Neg: 3634\n",
      "Vocab size: 3718\n",
      "Epoch 1 | Batch 1 | Loss: 1.12950 | Accuracy: 0.7520\n",
      "Epoch 1 | Batch 101 | Loss: 1.25779 | Accuracy: 0.7524\n",
      "Epoch 1 | Batch 201 | Loss: 1.29847 | Accuracy: 0.7531\n",
      "Epoch 1 | Batch 301 | Loss: 1.30247 | Accuracy: 0.7533\n",
      "Epoch 1 | Batch 401 | Loss: 1.03575 | Accuracy: 0.7545\n",
      "Epoch 1 | Batch 501 | Loss: 1.03560 | Accuracy: 0.7551\n",
      "Epoch 1 | Batch 601 | Loss: 1.26326 | Accuracy: 0.7557\n",
      "Epoch 1 | Batch 701 | Loss: 1.18416 | Accuracy: 0.7567\n",
      "Epoch 1 | Batch 801 | Loss: 1.11980 | Accuracy: 0.7583\n",
      "Epoch 1 | Batch 901 | Loss: 1.17252 | Accuracy: 0.7572\n",
      "Epoch 1 | Batch 1001 | Loss: 1.13458 | Accuracy: 0.7584\n",
      "Epoch 1 | Batch 1101 | Loss: 1.06928 | Accuracy: 0.7595\n",
      "Epoch 1 | Batch 1201 | Loss: 1.34963 | Accuracy: 0.7601\n",
      "Epoch 1 | Batch 1301 | Loss: 1.07403 | Accuracy: 0.7601\n",
      "Epoch 1 | Batch 1401 | Loss: 1.06813 | Accuracy: 0.7613\n",
      "Epoch 1 | Batch 1501 | Loss: 1.18009 | Accuracy: 0.7610\n",
      "Epoch 1 | Batch 1601 | Loss: 1.30889 | Accuracy: 0.7630\n",
      "Epoch 1 | Batch 1701 | Loss: 1.21545 | Accuracy: 0.7632\n",
      "Epoch 1 | Batch 1801 | Loss: 1.12360 | Accuracy: 0.7630\n",
      "Epoch 1 | Batch 1901 | Loss: 1.17780 | Accuracy: 0.7647\n",
      "Epoch 1 | Batch 2001 | Loss: 1.14393 | Accuracy: 0.7650\n",
      "Epoch 1 | Batch 2101 | Loss: 1.13836 | Accuracy: 0.7658\n",
      "Epoch 1 | Batch 2201 | Loss: 1.09191 | Accuracy: 0.7659\n",
      "Epoch 1 | Batch 2301 | Loss: 1.12344 | Accuracy: 0.7663\n",
      "Epoch 1 | Batch 2401 | Loss: 1.13140 | Accuracy: 0.7675\n",
      "Epoch 1 | Batch 2501 | Loss: 1.24594 | Accuracy: 0.7677\n",
      "Epoch 1 | Batch 2601 | Loss: 1.35193 | Accuracy: 0.7685\n",
      "Epoch 1 | Batch 2701 | Loss: 1.16168 | Accuracy: 0.7696\n",
      "Epoch 1 | Batch 2801 | Loss: 1.31124 | Accuracy: 0.7707\n",
      "Epoch 1 | Batch 2901 | Loss: 1.01319 | Accuracy: 0.7704\n",
      "Epoch 1 | Batch 3001 | Loss: 1.12308 | Accuracy: 0.7705\n",
      "Epoch 1 | Batch 3101 | Loss: 1.09392 | Accuracy: 0.7713\n",
      "Epoch 1 | Batch 3201 | Loss: 1.21825 | Accuracy: 0.7717\n",
      "Epoch 1 | Batch 3301 | Loss: 1.21153 | Accuracy: 0.7713\n",
      "Epoch 1 | Batch 3401 | Loss: 1.12676 | Accuracy: 0.7730\n",
      "Epoch 1 | Batch 3501 | Loss: 0.99557 | Accuracy: 0.7741\n",
      "Epoch 1 | Batch 3601 | Loss: 1.12583 | Accuracy: 0.7731\n",
      "Epoch 1 | Batch 3701 | Loss: 1.09305 | Accuracy: 0.7742\n",
      "Epoch 1 | Batch 3801 | Loss: 1.09081 | Accuracy: 0.7749\n",
      "Epoch 1 | Batch 3901 | Loss: 1.12450 | Accuracy: 0.7756\n",
      "Epoch 1 | Batch 4001 | Loss: 1.00965 | Accuracy: 0.7763\n",
      "Epoch 1 | Batch 4101 | Loss: 1.16450 | Accuracy: 0.7761\n",
      "Epoch 1 | Batch 4201 | Loss: 1.28516 | Accuracy: 0.7761\n",
      "Epoch 1 | Batch 4301 | Loss: 0.96515 | Accuracy: 0.7773\n",
      "Epoch 1 | Batch 4401 | Loss: 1.05615 | Accuracy: 0.7775\n",
      "Epoch 1 | Batch 4501 | Loss: 1.16614 | Accuracy: 0.7790\n",
      "Epoch 1 | Batch 4601 | Loss: 1.11209 | Accuracy: 0.7804\n",
      "Batch shapes - X: torch.Size([32]), Pos: torch.Size([32, 2]), Neg: torch.Size([32, 2])\n",
      "Max indices - X: 3042, Pos: 3710, Neg: 3714\n",
      "Vocab size: 3718\n",
      "Epoch 2 | Batch 1 | Loss: 1.12849 | Accuracy: 0.7792\n",
      "Epoch 2 | Batch 101 | Loss: 1.10279 | Accuracy: 0.7791\n",
      "Epoch 2 | Batch 201 | Loss: 1.05601 | Accuracy: 0.7800\n",
      "Epoch 2 | Batch 301 | Loss: 1.12772 | Accuracy: 0.7807\n",
      "Epoch 2 | Batch 401 | Loss: 1.31753 | Accuracy: 0.7814\n",
      "Epoch 2 | Batch 501 | Loss: 1.33837 | Accuracy: 0.7826\n",
      "Epoch 2 | Batch 601 | Loss: 1.22397 | Accuracy: 0.7816\n",
      "Epoch 2 | Batch 701 | Loss: 1.14944 | Accuracy: 0.7829\n",
      "Epoch 2 | Batch 801 | Loss: 1.18660 | Accuracy: 0.7826\n",
      "Epoch 2 | Batch 901 | Loss: 1.27664 | Accuracy: 0.7833\n",
      "Epoch 2 | Batch 1001 | Loss: 1.23915 | Accuracy: 0.7832\n",
      "Epoch 2 | Batch 1101 | Loss: 1.07384 | Accuracy: 0.7844\n",
      "Epoch 2 | Batch 1201 | Loss: 1.09331 | Accuracy: 0.7847\n",
      "Epoch 2 | Batch 1301 | Loss: 1.23014 | Accuracy: 0.7854\n",
      "Epoch 2 | Batch 1401 | Loss: 1.04325 | Accuracy: 0.7852\n",
      "Epoch 2 | Batch 1501 | Loss: 1.18387 | Accuracy: 0.7859\n",
      "Epoch 2 | Batch 1601 | Loss: 1.19198 | Accuracy: 0.7868\n",
      "Epoch 2 | Batch 1701 | Loss: 1.05040 | Accuracy: 0.7869\n",
      "Epoch 2 | Batch 1801 | Loss: 1.18443 | Accuracy: 0.7864\n",
      "Epoch 2 | Batch 1901 | Loss: 0.95838 | Accuracy: 0.7881\n",
      "Epoch 2 | Batch 2001 | Loss: 1.06834 | Accuracy: 0.7881\n",
      "Epoch 2 | Batch 2101 | Loss: 1.05004 | Accuracy: 0.7878\n",
      "Epoch 2 | Batch 2201 | Loss: 1.05031 | Accuracy: 0.7888\n",
      "Epoch 2 | Batch 2301 | Loss: 1.13655 | Accuracy: 0.7891\n",
      "Epoch 2 | Batch 2401 | Loss: 1.12279 | Accuracy: 0.7896\n",
      "Epoch 2 | Batch 2501 | Loss: 1.05817 | Accuracy: 0.7901\n",
      "Epoch 2 | Batch 2601 | Loss: 1.13459 | Accuracy: 0.7904\n",
      "Epoch 2 | Batch 2701 | Loss: 1.18060 | Accuracy: 0.7906\n",
      "Epoch 2 | Batch 2801 | Loss: 1.17606 | Accuracy: 0.7912\n",
      "Epoch 2 | Batch 2901 | Loss: 1.08290 | Accuracy: 0.7922\n",
      "Epoch 2 | Batch 3001 | Loss: 1.29663 | Accuracy: 0.7915\n",
      "Epoch 2 | Batch 3101 | Loss: 1.11065 | Accuracy: 0.7923\n",
      "Epoch 2 | Batch 3201 | Loss: 1.18249 | Accuracy: 0.7924\n",
      "Epoch 2 | Batch 3301 | Loss: 1.08076 | Accuracy: 0.7929\n",
      "Epoch 2 | Batch 3401 | Loss: 1.11214 | Accuracy: 0.7931\n",
      "Epoch 2 | Batch 3501 | Loss: 1.06672 | Accuracy: 0.7929\n",
      "Epoch 2 | Batch 3601 | Loss: 1.14339 | Accuracy: 0.7944\n",
      "Epoch 2 | Batch 3701 | Loss: 1.14501 | Accuracy: 0.7939\n",
      "Epoch 2 | Batch 3801 | Loss: 1.09184 | Accuracy: 0.7953\n",
      "Epoch 2 | Batch 3901 | Loss: 1.06901 | Accuracy: 0.7950\n",
      "Epoch 2 | Batch 4001 | Loss: 1.03646 | Accuracy: 0.7956\n",
      "Epoch 2 | Batch 4101 | Loss: 1.02471 | Accuracy: 0.7956\n",
      "Epoch 2 | Batch 4201 | Loss: 1.09759 | Accuracy: 0.7970\n",
      "Epoch 2 | Batch 4301 | Loss: 1.00171 | Accuracy: 0.7969\n",
      "Epoch 2 | Batch 4401 | Loss: 1.03576 | Accuracy: 0.7977\n",
      "Epoch 2 | Batch 4501 | Loss: 1.10791 | Accuracy: 0.7972\n",
      "Epoch 2 | Batch 4601 | Loss: 1.11663 | Accuracy: 0.7980\n",
      "Batch shapes - X: torch.Size([32]), Pos: torch.Size([32, 2]), Neg: torch.Size([32, 2])\n",
      "Max indices - X: 3684, Pos: 3123, Neg: 3693\n",
      "Vocab size: 3718\n",
      "Epoch 3 | Batch 1 | Loss: 1.09988 | Accuracy: 0.7975\n",
      "Epoch 3 | Batch 101 | Loss: 1.06685 | Accuracy: 0.7983\n",
      "Epoch 3 | Batch 201 | Loss: 1.09386 | Accuracy: 0.7992\n",
      "Epoch 3 | Batch 301 | Loss: 1.02400 | Accuracy: 0.7984\n",
      "Epoch 3 | Batch 401 | Loss: 1.25763 | Accuracy: 0.7996\n",
      "Epoch 3 | Batch 501 | Loss: 1.11900 | Accuracy: 0.7996\n",
      "Epoch 3 | Batch 601 | Loss: 1.18924 | Accuracy: 0.8004\n",
      "Epoch 3 | Batch 701 | Loss: 1.10435 | Accuracy: 0.7996\n",
      "Epoch 3 | Batch 801 | Loss: 1.08739 | Accuracy: 0.8008\n",
      "Epoch 3 | Batch 901 | Loss: 1.24268 | Accuracy: 0.8008\n",
      "Epoch 3 | Batch 1001 | Loss: 0.99733 | Accuracy: 0.8010\n",
      "Epoch 3 | Batch 1101 | Loss: 1.00080 | Accuracy: 0.8007\n",
      "Epoch 3 | Batch 1201 | Loss: 1.05865 | Accuracy: 0.8018\n",
      "Epoch 3 | Batch 1301 | Loss: 1.01458 | Accuracy: 0.8024\n",
      "Epoch 3 | Batch 1401 | Loss: 0.97097 | Accuracy: 0.8022\n",
      "Epoch 3 | Batch 1501 | Loss: 1.04477 | Accuracy: 0.8023\n",
      "Epoch 3 | Batch 1601 | Loss: 1.16077 | Accuracy: 0.8029\n",
      "Epoch 3 | Batch 1701 | Loss: 0.99491 | Accuracy: 0.8021\n",
      "Epoch 3 | Batch 1801 | Loss: 0.99964 | Accuracy: 0.8034\n",
      "Epoch 3 | Batch 1901 | Loss: 1.00946 | Accuracy: 0.8047\n",
      "Epoch 3 | Batch 2001 | Loss: 1.03398 | Accuracy: 0.8053\n",
      "Epoch 3 | Batch 2101 | Loss: 1.09053 | Accuracy: 0.8045\n",
      "Epoch 3 | Batch 2201 | Loss: 1.10895 | Accuracy: 0.8049\n",
      "Epoch 3 | Batch 2301 | Loss: 1.09848 | Accuracy: 0.8046\n",
      "Epoch 3 | Batch 2401 | Loss: 1.12460 | Accuracy: 0.8057\n",
      "Epoch 3 | Batch 2501 | Loss: 1.06177 | Accuracy: 0.8065\n",
      "Epoch 3 | Batch 2601 | Loss: 0.96006 | Accuracy: 0.8052\n",
      "Epoch 3 | Batch 2701 | Loss: 1.16780 | Accuracy: 0.8057\n",
      "Epoch 3 | Batch 2801 | Loss: 1.07138 | Accuracy: 0.8064\n",
      "Epoch 3 | Batch 2901 | Loss: 1.09024 | Accuracy: 0.8060\n",
      "Epoch 3 | Batch 3001 | Loss: 1.17270 | Accuracy: 0.8076\n",
      "Epoch 3 | Batch 3101 | Loss: 1.05457 | Accuracy: 0.8075\n",
      "Epoch 3 | Batch 3201 | Loss: 1.00518 | Accuracy: 0.8081\n",
      "Epoch 3 | Batch 3301 | Loss: 1.07117 | Accuracy: 0.8075\n",
      "Epoch 3 | Batch 3401 | Loss: 1.08296 | Accuracy: 0.8082\n",
      "Epoch 3 | Batch 3501 | Loss: 1.01766 | Accuracy: 0.8089\n",
      "Epoch 3 | Batch 3601 | Loss: 1.15845 | Accuracy: 0.8088\n",
      "Epoch 3 | Batch 3701 | Loss: 0.98678 | Accuracy: 0.8094\n",
      "Epoch 3 | Batch 3801 | Loss: 1.06522 | Accuracy: 0.8094\n",
      "Epoch 3 | Batch 3901 | Loss: 1.11614 | Accuracy: 0.8099\n",
      "Epoch 3 | Batch 4001 | Loss: 0.86941 | Accuracy: 0.8099\n",
      "Epoch 3 | Batch 4101 | Loss: 1.01256 | Accuracy: 0.8110\n",
      "Epoch 3 | Batch 4201 | Loss: 1.16527 | Accuracy: 0.8104\n",
      "Epoch 3 | Batch 4301 | Loss: 1.14908 | Accuracy: 0.8112\n",
      "Epoch 3 | Batch 4401 | Loss: 1.00330 | Accuracy: 0.8113\n",
      "Epoch 3 | Batch 4501 | Loss: 1.05539 | Accuracy: 0.8109\n",
      "Epoch 3 | Batch 4601 | Loss: 0.98517 | Accuracy: 0.8117\n",
      "Batch shapes - X: torch.Size([32]), Pos: torch.Size([32, 2]), Neg: torch.Size([32, 2])\n",
      "Max indices - X: 2031, Pos: 3634, Neg: 3717\n",
      "Vocab size: 3718\n",
      "Epoch 4 | Batch 1 | Loss: 1.02679 | Accuracy: 0.8121\n",
      "Epoch 4 | Batch 101 | Loss: 1.14121 | Accuracy: 0.8119\n",
      "Epoch 4 | Batch 201 | Loss: 0.99629 | Accuracy: 0.8121\n",
      "Epoch 4 | Batch 301 | Loss: 1.13161 | Accuracy: 0.8126\n",
      "Epoch 4 | Batch 401 | Loss: 1.10977 | Accuracy: 0.8131\n",
      "Epoch 4 | Batch 501 | Loss: 1.11726 | Accuracy: 0.8127\n",
      "Epoch 4 | Batch 601 | Loss: 1.19679 | Accuracy: 0.8126\n",
      "Epoch 4 | Batch 701 | Loss: 1.17092 | Accuracy: 0.8136\n",
      "Epoch 4 | Batch 801 | Loss: 1.14643 | Accuracy: 0.8139\n",
      "Epoch 4 | Batch 901 | Loss: 1.08719 | Accuracy: 0.8137\n",
      "Epoch 4 | Batch 1001 | Loss: 0.88613 | Accuracy: 0.8139\n",
      "Epoch 4 | Batch 1101 | Loss: 0.94302 | Accuracy: 0.8141\n",
      "Epoch 4 | Batch 1201 | Loss: 0.95401 | Accuracy: 0.8150\n",
      "Epoch 4 | Batch 1301 | Loss: 1.13137 | Accuracy: 0.8155\n",
      "Epoch 4 | Batch 1401 | Loss: 1.04047 | Accuracy: 0.8143\n",
      "Epoch 4 | Batch 1501 | Loss: 1.08979 | Accuracy: 0.8153\n",
      "Epoch 4 | Batch 1601 | Loss: 1.11841 | Accuracy: 0.8151\n",
      "Epoch 4 | Batch 1701 | Loss: 0.92504 | Accuracy: 0.8161\n",
      "Epoch 4 | Batch 1801 | Loss: 1.18373 | Accuracy: 0.8160\n",
      "Epoch 4 | Batch 1901 | Loss: 1.01674 | Accuracy: 0.8163\n",
      "Epoch 4 | Batch 2001 | Loss: 1.01606 | Accuracy: 0.8168\n",
      "Epoch 4 | Batch 2101 | Loss: 0.98909 | Accuracy: 0.8171\n",
      "Epoch 4 | Batch 2201 | Loss: 0.94058 | Accuracy: 0.8175\n",
      "Epoch 4 | Batch 2301 | Loss: 1.05588 | Accuracy: 0.8163\n",
      "Epoch 4 | Batch 2401 | Loss: 1.07336 | Accuracy: 0.8169\n",
      "Epoch 4 | Batch 2501 | Loss: 0.97964 | Accuracy: 0.8180\n",
      "Epoch 4 | Batch 2601 | Loss: 1.02503 | Accuracy: 0.8171\n",
      "Epoch 4 | Batch 2701 | Loss: 1.03364 | Accuracy: 0.8186\n",
      "Epoch 4 | Batch 2801 | Loss: 0.97680 | Accuracy: 0.8185\n",
      "Epoch 4 | Batch 2901 | Loss: 0.98352 | Accuracy: 0.8187\n",
      "Epoch 4 | Batch 3001 | Loss: 1.30028 | Accuracy: 0.8182\n",
      "Epoch 4 | Batch 3101 | Loss: 1.08134 | Accuracy: 0.8189\n",
      "Epoch 4 | Batch 3201 | Loss: 0.87565 | Accuracy: 0.8191\n",
      "Epoch 4 | Batch 3301 | Loss: 1.06924 | Accuracy: 0.8193\n",
      "Epoch 4 | Batch 3401 | Loss: 1.13254 | Accuracy: 0.8198\n",
      "Epoch 4 | Batch 3501 | Loss: 0.96676 | Accuracy: 0.8187\n",
      "Epoch 4 | Batch 3601 | Loss: 0.85786 | Accuracy: 0.8209\n",
      "Epoch 4 | Batch 3701 | Loss: 1.06444 | Accuracy: 0.8197\n",
      "Epoch 4 | Batch 3801 | Loss: 1.02097 | Accuracy: 0.8199\n",
      "Epoch 4 | Batch 3901 | Loss: 1.09468 | Accuracy: 0.8205\n",
      "Epoch 4 | Batch 4001 | Loss: 1.09747 | Accuracy: 0.8201\n",
      "Epoch 4 | Batch 4101 | Loss: 1.01404 | Accuracy: 0.8201\n",
      "Epoch 4 | Batch 4201 | Loss: 0.87958 | Accuracy: 0.8216\n",
      "Epoch 4 | Batch 4301 | Loss: 1.10533 | Accuracy: 0.8218\n",
      "Epoch 4 | Batch 4401 | Loss: 0.99845 | Accuracy: 0.8222\n",
      "Epoch 4 | Batch 4501 | Loss: 0.90509 | Accuracy: 0.8212\n",
      "Epoch 4 | Batch 4601 | Loss: 1.06201 | Accuracy: 0.8229\n",
      "Batch shapes - X: torch.Size([32]), Pos: torch.Size([32, 2]), Neg: torch.Size([32, 2])\n",
      "Max indices - X: 2755, Pos: 3703, Neg: 3653\n",
      "Vocab size: 3718\n",
      "Epoch 5 | Batch 1 | Loss: 0.96876 | Accuracy: 0.8229\n",
      "Epoch 5 | Batch 101 | Loss: 1.01213 | Accuracy: 0.8221\n",
      "Epoch 5 | Batch 201 | Loss: 0.99124 | Accuracy: 0.8229\n",
      "Epoch 5 | Batch 301 | Loss: 0.96115 | Accuracy: 0.8229\n",
      "Epoch 5 | Batch 401 | Loss: 1.05422 | Accuracy: 0.8220\n",
      "Epoch 5 | Batch 501 | Loss: 0.81109 | Accuracy: 0.8227\n",
      "Epoch 5 | Batch 601 | Loss: 1.13020 | Accuracy: 0.8233\n",
      "Epoch 5 | Batch 701 | Loss: 0.89503 | Accuracy: 0.8236\n",
      "Epoch 5 | Batch 801 | Loss: 1.03615 | Accuracy: 0.8243\n",
      "Epoch 5 | Batch 901 | Loss: 1.04689 | Accuracy: 0.8243\n",
      "Epoch 5 | Batch 1001 | Loss: 1.03559 | Accuracy: 0.8240\n",
      "Epoch 5 | Batch 1101 | Loss: 1.04162 | Accuracy: 0.8242\n",
      "Epoch 5 | Batch 1201 | Loss: 1.01780 | Accuracy: 0.8240\n",
      "Epoch 5 | Batch 1301 | Loss: 1.11675 | Accuracy: 0.8247\n",
      "Epoch 5 | Batch 1401 | Loss: 0.90605 | Accuracy: 0.8241\n",
      "Epoch 5 | Batch 1501 | Loss: 1.01114 | Accuracy: 0.8259\n",
      "Epoch 5 | Batch 1601 | Loss: 1.05109 | Accuracy: 0.8256\n",
      "Epoch 5 | Batch 1701 | Loss: 0.87490 | Accuracy: 0.8266\n",
      "Epoch 5 | Batch 1801 | Loss: 1.08234 | Accuracy: 0.8263\n",
      "Epoch 5 | Batch 1901 | Loss: 1.12507 | Accuracy: 0.8257\n",
      "Epoch 5 | Batch 2001 | Loss: 0.95843 | Accuracy: 0.8261\n",
      "Epoch 5 | Batch 2101 | Loss: 1.01448 | Accuracy: 0.8263\n",
      "Epoch 5 | Batch 2201 | Loss: 1.03477 | Accuracy: 0.8269\n",
      "Epoch 5 | Batch 2301 | Loss: 0.94898 | Accuracy: 0.8275\n",
      "Epoch 5 | Batch 2401 | Loss: 0.93271 | Accuracy: 0.8273\n",
      "Epoch 5 | Batch 2501 | Loss: 0.90100 | Accuracy: 0.8270\n",
      "Epoch 5 | Batch 2601 | Loss: 1.10045 | Accuracy: 0.8269\n",
      "Epoch 5 | Batch 2701 | Loss: 1.12521 | Accuracy: 0.8276\n",
      "Epoch 5 | Batch 2801 | Loss: 1.01871 | Accuracy: 0.8277\n",
      "Epoch 5 | Batch 2901 | Loss: 1.11678 | Accuracy: 0.8270\n",
      "Epoch 5 | Batch 3001 | Loss: 0.99006 | Accuracy: 0.8281\n",
      "Epoch 5 | Batch 3101 | Loss: 0.96096 | Accuracy: 0.8276\n",
      "Epoch 5 | Batch 3201 | Loss: 0.92363 | Accuracy: 0.8286\n",
      "Epoch 5 | Batch 3301 | Loss: 1.05798 | Accuracy: 0.8275\n",
      "Epoch 5 | Batch 3401 | Loss: 0.95465 | Accuracy: 0.8286\n",
      "Epoch 5 | Batch 3501 | Loss: 0.87979 | Accuracy: 0.8287\n",
      "Epoch 5 | Batch 3601 | Loss: 1.04866 | Accuracy: 0.8303\n",
      "Epoch 5 | Batch 3701 | Loss: 1.01918 | Accuracy: 0.8293\n",
      "Epoch 5 | Batch 3801 | Loss: 1.04696 | Accuracy: 0.8293\n",
      "Epoch 5 | Batch 3901 | Loss: 0.98559 | Accuracy: 0.8295\n",
      "Epoch 5 | Batch 4001 | Loss: 0.95425 | Accuracy: 0.8299\n",
      "Epoch 5 | Batch 4101 | Loss: 1.13611 | Accuracy: 0.8306\n",
      "Epoch 5 | Batch 4201 | Loss: 1.01642 | Accuracy: 0.8297\n",
      "Epoch 5 | Batch 4301 | Loss: 1.06175 | Accuracy: 0.8305\n",
      "Epoch 5 | Batch 4401 | Loss: 0.97372 | Accuracy: 0.8304\n",
      "Epoch 5 | Batch 4501 | Loss: 0.97381 | Accuracy: 0.8304\n",
      "Epoch 5 | Batch 4601 | Loss: 1.08220 | Accuracy: 0.8315\n",
      "Batch shapes - X: torch.Size([32]), Pos: torch.Size([32, 2]), Neg: torch.Size([32, 2])\n",
      "Max indices - X: 2451, Pos: 3635, Neg: 3687\n",
      "Vocab size: 3718\n",
      "Epoch 6 | Batch 1 | Loss: 0.92702 | Accuracy: 0.8301\n",
      "Epoch 6 | Batch 101 | Loss: 0.81189 | Accuracy: 0.8308\n",
      "Epoch 6 | Batch 201 | Loss: 1.01978 | Accuracy: 0.8305\n",
      "Epoch 6 | Batch 301 | Loss: 0.99063 | Accuracy: 0.8314\n",
      "Epoch 6 | Batch 401 | Loss: 0.91523 | Accuracy: 0.8309\n",
      "Epoch 6 | Batch 501 | Loss: 1.00983 | Accuracy: 0.8318\n",
      "Epoch 6 | Batch 601 | Loss: 1.02240 | Accuracy: 0.8322\n",
      "Epoch 6 | Batch 701 | Loss: 1.18620 | Accuracy: 0.8326\n",
      "Epoch 6 | Batch 801 | Loss: 0.95195 | Accuracy: 0.8320\n",
      "Epoch 6 | Batch 901 | Loss: 0.96034 | Accuracy: 0.8323\n",
      "Epoch 6 | Batch 1001 | Loss: 0.91406 | Accuracy: 0.8324\n",
      "Epoch 6 | Batch 1101 | Loss: 0.89774 | Accuracy: 0.8333\n",
      "Epoch 6 | Batch 1201 | Loss: 1.16001 | Accuracy: 0.8332\n",
      "Epoch 6 | Batch 1301 | Loss: 0.94157 | Accuracy: 0.8326\n",
      "Epoch 6 | Batch 1401 | Loss: 1.01429 | Accuracy: 0.8331\n",
      "Epoch 6 | Batch 1501 | Loss: 0.96070 | Accuracy: 0.8328\n",
      "Epoch 6 | Batch 1601 | Loss: 1.08288 | Accuracy: 0.8335\n",
      "Epoch 6 | Batch 1701 | Loss: 1.13437 | Accuracy: 0.8331\n",
      "Epoch 6 | Batch 1801 | Loss: 1.03685 | Accuracy: 0.8327\n",
      "Epoch 6 | Batch 1901 | Loss: 0.90624 | Accuracy: 0.8337\n",
      "Epoch 6 | Batch 2001 | Loss: 1.05656 | Accuracy: 0.8343\n",
      "Epoch 6 | Batch 2101 | Loss: 1.02496 | Accuracy: 0.8341\n",
      "Epoch 6 | Batch 2201 | Loss: 0.94887 | Accuracy: 0.8347\n",
      "Epoch 6 | Batch 2301 | Loss: 0.87413 | Accuracy: 0.8343\n",
      "Epoch 6 | Batch 2401 | Loss: 1.01050 | Accuracy: 0.8349\n",
      "Epoch 6 | Batch 2501 | Loss: 0.93274 | Accuracy: 0.8345\n",
      "Epoch 6 | Batch 2601 | Loss: 0.85492 | Accuracy: 0.8342\n",
      "Epoch 6 | Batch 2701 | Loss: 1.01081 | Accuracy: 0.8347\n",
      "Epoch 6 | Batch 2801 | Loss: 1.05800 | Accuracy: 0.8355\n",
      "Epoch 6 | Batch 2901 | Loss: 0.86612 | Accuracy: 0.8357\n",
      "Epoch 6 | Batch 3001 | Loss: 1.05649 | Accuracy: 0.8360\n",
      "Epoch 6 | Batch 3101 | Loss: 1.05167 | Accuracy: 0.8358\n",
      "Epoch 6 | Batch 3201 | Loss: 1.04341 | Accuracy: 0.8369\n",
      "Epoch 6 | Batch 3301 | Loss: 1.00279 | Accuracy: 0.8352\n",
      "Epoch 6 | Batch 3401 | Loss: 1.03021 | Accuracy: 0.8359\n",
      "Epoch 6 | Batch 3501 | Loss: 0.91770 | Accuracy: 0.8355\n",
      "Epoch 6 | Batch 3601 | Loss: 0.98265 | Accuracy: 0.8367\n",
      "Epoch 6 | Batch 3701 | Loss: 0.83776 | Accuracy: 0.8366\n",
      "Epoch 6 | Batch 3801 | Loss: 0.94825 | Accuracy: 0.8366\n",
      "Epoch 6 | Batch 3901 | Loss: 1.19879 | Accuracy: 0.8364\n",
      "Epoch 6 | Batch 4001 | Loss: 0.93357 | Accuracy: 0.8371\n",
      "Epoch 6 | Batch 4101 | Loss: 0.97993 | Accuracy: 0.8373\n",
      "Epoch 6 | Batch 4201 | Loss: 0.98573 | Accuracy: 0.8377\n",
      "Epoch 6 | Batch 4301 | Loss: 1.01838 | Accuracy: 0.8383\n",
      "Epoch 6 | Batch 4401 | Loss: 0.86415 | Accuracy: 0.8368\n",
      "Epoch 6 | Batch 4501 | Loss: 0.95265 | Accuracy: 0.8381\n",
      "Epoch 6 | Batch 4601 | Loss: 1.04135 | Accuracy: 0.8376\n",
      "Batch shapes - X: torch.Size([32]), Pos: torch.Size([32, 2]), Neg: torch.Size([32, 2])\n",
      "Max indices - X: 2574, Pos: 3714, Neg: 3701\n",
      "Vocab size: 3718\n",
      "Epoch 7 | Batch 1 | Loss: 0.84603 | Accuracy: 0.8380\n",
      "Epoch 7 | Batch 101 | Loss: 0.93478 | Accuracy: 0.8387\n",
      "Epoch 7 | Batch 201 | Loss: 1.02534 | Accuracy: 0.8382\n",
      "Epoch 7 | Batch 301 | Loss: 0.92375 | Accuracy: 0.8383\n",
      "Epoch 7 | Batch 401 | Loss: 1.05139 | Accuracy: 0.8378\n",
      "Epoch 7 | Batch 501 | Loss: 0.86768 | Accuracy: 0.8380\n",
      "Epoch 7 | Batch 601 | Loss: 0.99233 | Accuracy: 0.8389\n",
      "Epoch 7 | Batch 701 | Loss: 1.12657 | Accuracy: 0.8388\n",
      "Epoch 7 | Batch 801 | Loss: 0.86320 | Accuracy: 0.8381\n",
      "Epoch 7 | Batch 901 | Loss: 1.01484 | Accuracy: 0.8387\n",
      "Epoch 7 | Batch 1001 | Loss: 1.03878 | Accuracy: 0.8381\n",
      "Epoch 7 | Batch 1101 | Loss: 0.87695 | Accuracy: 0.8398\n",
      "Epoch 7 | Batch 1201 | Loss: 0.89951 | Accuracy: 0.8396\n",
      "Epoch 7 | Batch 1301 | Loss: 0.88185 | Accuracy: 0.8390\n",
      "Epoch 7 | Batch 1401 | Loss: 1.01509 | Accuracy: 0.8392\n",
      "Epoch 7 | Batch 1501 | Loss: 1.17374 | Accuracy: 0.8404\n",
      "Epoch 7 | Batch 1601 | Loss: 0.81412 | Accuracy: 0.8404\n",
      "Epoch 7 | Batch 1701 | Loss: 1.02789 | Accuracy: 0.8402\n",
      "Epoch 7 | Batch 1801 | Loss: 1.20230 | Accuracy: 0.8405\n",
      "Epoch 7 | Batch 1901 | Loss: 0.95804 | Accuracy: 0.8400\n",
      "Epoch 7 | Batch 2001 | Loss: 0.91905 | Accuracy: 0.8401\n",
      "Epoch 7 | Batch 2101 | Loss: 1.04648 | Accuracy: 0.8403\n",
      "Epoch 7 | Batch 2201 | Loss: 0.90961 | Accuracy: 0.8411\n",
      "Epoch 7 | Batch 2301 | Loss: 1.01889 | Accuracy: 0.8411\n",
      "Epoch 7 | Batch 2401 | Loss: 0.94013 | Accuracy: 0.8404\n",
      "Epoch 7 | Batch 2501 | Loss: 0.96778 | Accuracy: 0.8416\n",
      "Epoch 7 | Batch 2601 | Loss: 1.20962 | Accuracy: 0.8412\n",
      "Epoch 7 | Batch 2701 | Loss: 1.03599 | Accuracy: 0.8408\n",
      "Epoch 7 | Batch 2801 | Loss: 1.01762 | Accuracy: 0.8416\n",
      "Epoch 7 | Batch 2901 | Loss: 1.20113 | Accuracy: 0.8421\n",
      "Epoch 7 | Batch 3001 | Loss: 1.13027 | Accuracy: 0.8420\n",
      "Epoch 7 | Batch 3101 | Loss: 0.90491 | Accuracy: 0.8423\n",
      "Epoch 7 | Batch 3201 | Loss: 1.00887 | Accuracy: 0.8422\n",
      "Epoch 7 | Batch 3301 | Loss: 1.03986 | Accuracy: 0.8422\n",
      "Epoch 7 | Batch 3401 | Loss: 0.91090 | Accuracy: 0.8425\n",
      "Epoch 7 | Batch 3501 | Loss: 0.87124 | Accuracy: 0.8415\n",
      "Epoch 7 | Batch 3601 | Loss: 1.01773 | Accuracy: 0.8418\n",
      "Epoch 7 | Batch 3701 | Loss: 0.98569 | Accuracy: 0.8426\n",
      "Epoch 7 | Batch 3801 | Loss: 0.95749 | Accuracy: 0.8426\n",
      "Epoch 7 | Batch 3901 | Loss: 0.98547 | Accuracy: 0.8427\n",
      "Epoch 7 | Batch 4001 | Loss: 0.91460 | Accuracy: 0.8431\n",
      "Epoch 7 | Batch 4101 | Loss: 0.90970 | Accuracy: 0.8433\n",
      "Epoch 7 | Batch 4201 | Loss: 0.78710 | Accuracy: 0.8429\n",
      "Epoch 7 | Batch 4301 | Loss: 1.01072 | Accuracy: 0.8435\n",
      "Epoch 7 | Batch 4401 | Loss: 1.08519 | Accuracy: 0.8438\n",
      "Epoch 7 | Batch 4501 | Loss: 0.90480 | Accuracy: 0.8437\n",
      "Epoch 7 | Batch 4601 | Loss: 1.05974 | Accuracy: 0.8441\n",
      "Batch shapes - X: torch.Size([32]), Pos: torch.Size([32, 2]), Neg: torch.Size([32, 2])\n",
      "Max indices - X: 3651, Pos: 3252, Neg: 3689\n",
      "Vocab size: 3718\n",
      "Epoch 8 | Batch 1 | Loss: 1.12640 | Accuracy: 0.8438\n",
      "Epoch 8 | Batch 101 | Loss: 1.00602 | Accuracy: 0.8438\n",
      "Epoch 8 | Batch 201 | Loss: 0.85514 | Accuracy: 0.8439\n",
      "Epoch 8 | Batch 301 | Loss: 0.98601 | Accuracy: 0.8447\n",
      "Epoch 8 | Batch 401 | Loss: 0.95360 | Accuracy: 0.8443\n",
      "Epoch 8 | Batch 501 | Loss: 0.95429 | Accuracy: 0.8446\n",
      "Epoch 8 | Batch 601 | Loss: 0.88216 | Accuracy: 0.8447\n",
      "Epoch 8 | Batch 701 | Loss: 0.94688 | Accuracy: 0.8449\n",
      "Epoch 8 | Batch 801 | Loss: 0.81391 | Accuracy: 0.8461\n",
      "Epoch 8 | Batch 901 | Loss: 1.06254 | Accuracy: 0.8436\n",
      "Epoch 8 | Batch 1001 | Loss: 0.94075 | Accuracy: 0.8454\n",
      "Epoch 8 | Batch 1101 | Loss: 0.95548 | Accuracy: 0.8449\n",
      "Epoch 8 | Batch 1201 | Loss: 1.04769 | Accuracy: 0.8451\n",
      "Epoch 8 | Batch 1301 | Loss: 1.14848 | Accuracy: 0.8450\n",
      "Epoch 8 | Batch 1401 | Loss: 0.87571 | Accuracy: 0.8455\n",
      "Epoch 8 | Batch 1501 | Loss: 0.88368 | Accuracy: 0.8463\n",
      "Epoch 8 | Batch 1601 | Loss: 0.93023 | Accuracy: 0.8459\n",
      "Epoch 8 | Batch 1701 | Loss: 1.02319 | Accuracy: 0.8464\n",
      "Epoch 8 | Batch 1801 | Loss: 0.95191 | Accuracy: 0.8455\n",
      "Epoch 8 | Batch 1901 | Loss: 1.00332 | Accuracy: 0.8461\n",
      "Epoch 8 | Batch 2001 | Loss: 1.01463 | Accuracy: 0.8463\n",
      "Epoch 8 | Batch 2101 | Loss: 0.98309 | Accuracy: 0.8472\n",
      "Epoch 8 | Batch 2201 | Loss: 1.05259 | Accuracy: 0.8464\n",
      "Epoch 8 | Batch 2301 | Loss: 0.93704 | Accuracy: 0.8465\n",
      "Epoch 8 | Batch 2401 | Loss: 0.97331 | Accuracy: 0.8464\n",
      "Epoch 8 | Batch 2501 | Loss: 0.92773 | Accuracy: 0.8463\n",
      "Epoch 8 | Batch 2601 | Loss: 0.90360 | Accuracy: 0.8463\n",
      "Epoch 8 | Batch 2701 | Loss: 0.88352 | Accuracy: 0.8464\n",
      "Epoch 8 | Batch 2801 | Loss: 1.10239 | Accuracy: 0.8468\n",
      "Epoch 8 | Batch 2901 | Loss: 1.07472 | Accuracy: 0.8474\n",
      "Epoch 8 | Batch 3001 | Loss: 0.92167 | Accuracy: 0.8468\n",
      "Epoch 8 | Batch 3101 | Loss: 0.98191 | Accuracy: 0.8468\n",
      "Epoch 8 | Batch 3201 | Loss: 0.90536 | Accuracy: 0.8481\n",
      "Epoch 8 | Batch 3301 | Loss: 1.03711 | Accuracy: 0.8484\n",
      "Epoch 8 | Batch 3401 | Loss: 0.96969 | Accuracy: 0.8480\n",
      "Epoch 8 | Batch 3501 | Loss: 0.94643 | Accuracy: 0.8476\n",
      "Epoch 8 | Batch 3601 | Loss: 0.87351 | Accuracy: 0.8471\n",
      "Epoch 8 | Batch 3701 | Loss: 0.92758 | Accuracy: 0.8478\n",
      "Epoch 8 | Batch 3801 | Loss: 1.00582 | Accuracy: 0.8474\n",
      "Epoch 8 | Batch 3901 | Loss: 0.94886 | Accuracy: 0.8487\n",
      "Epoch 8 | Batch 4001 | Loss: 1.02194 | Accuracy: 0.8483\n",
      "Epoch 8 | Batch 4101 | Loss: 0.99694 | Accuracy: 0.8485\n",
      "Epoch 8 | Batch 4201 | Loss: 0.99953 | Accuracy: 0.8485\n",
      "Epoch 8 | Batch 4301 | Loss: 1.01936 | Accuracy: 0.8489\n",
      "Epoch 8 | Batch 4401 | Loss: 0.90719 | Accuracy: 0.8483\n",
      "Epoch 8 | Batch 4501 | Loss: 1.02123 | Accuracy: 0.8489\n",
      "Epoch 8 | Batch 4601 | Loss: 0.99560 | Accuracy: 0.8494\n",
      "Batch shapes - X: torch.Size([32]), Pos: torch.Size([32, 2]), Neg: torch.Size([32, 2])\n",
      "Max indices - X: 2711, Pos: 3432, Neg: 3697\n",
      "Vocab size: 3718\n",
      "Epoch 9 | Batch 1 | Loss: 1.02811 | Accuracy: 0.8492\n",
      "Epoch 9 | Batch 101 | Loss: 0.89774 | Accuracy: 0.8494\n",
      "Epoch 9 | Batch 201 | Loss: 0.90240 | Accuracy: 0.8493\n",
      "Epoch 9 | Batch 301 | Loss: 0.98165 | Accuracy: 0.8495\n",
      "Epoch 9 | Batch 401 | Loss: 0.90275 | Accuracy: 0.8503\n",
      "Epoch 9 | Batch 501 | Loss: 0.99167 | Accuracy: 0.8498\n",
      "Epoch 9 | Batch 601 | Loss: 0.85136 | Accuracy: 0.8498\n",
      "Epoch 9 | Batch 701 | Loss: 0.88064 | Accuracy: 0.8492\n",
      "Epoch 9 | Batch 801 | Loss: 0.92003 | Accuracy: 0.8502\n",
      "Epoch 9 | Batch 901 | Loss: 0.87116 | Accuracy: 0.8507\n",
      "Epoch 9 | Batch 1001 | Loss: 0.94880 | Accuracy: 0.8497\n",
      "Epoch 9 | Batch 1101 | Loss: 1.07892 | Accuracy: 0.8519\n",
      "Epoch 9 | Batch 1201 | Loss: 0.75039 | Accuracy: 0.8499\n",
      "Epoch 9 | Batch 1301 | Loss: 0.86780 | Accuracy: 0.8509\n",
      "Epoch 9 | Batch 1401 | Loss: 1.07937 | Accuracy: 0.8513\n",
      "Epoch 9 | Batch 1501 | Loss: 0.90130 | Accuracy: 0.8507\n",
      "Epoch 9 | Batch 1601 | Loss: 1.06346 | Accuracy: 0.8510\n",
      "Epoch 9 | Batch 1701 | Loss: 1.10077 | Accuracy: 0.8510\n",
      "Epoch 9 | Batch 1801 | Loss: 0.97300 | Accuracy: 0.8510\n",
      "Epoch 9 | Batch 1901 | Loss: 0.87073 | Accuracy: 0.8515\n",
      "Epoch 9 | Batch 2001 | Loss: 0.88895 | Accuracy: 0.8517\n",
      "Epoch 9 | Batch 2101 | Loss: 0.70928 | Accuracy: 0.8500\n",
      "Epoch 9 | Batch 2201 | Loss: 0.96695 | Accuracy: 0.8510\n",
      "Epoch 9 | Batch 2301 | Loss: 0.87153 | Accuracy: 0.8515\n",
      "Epoch 9 | Batch 2401 | Loss: 1.02228 | Accuracy: 0.8521\n",
      "Epoch 9 | Batch 2501 | Loss: 0.92288 | Accuracy: 0.8522\n",
      "Epoch 9 | Batch 2601 | Loss: 1.02437 | Accuracy: 0.8527\n",
      "Epoch 9 | Batch 2701 | Loss: 0.83427 | Accuracy: 0.8513\n",
      "Epoch 9 | Batch 2801 | Loss: 1.11903 | Accuracy: 0.8516\n",
      "Epoch 9 | Batch 2901 | Loss: 1.04837 | Accuracy: 0.8516\n",
      "Epoch 9 | Batch 3001 | Loss: 1.04912 | Accuracy: 0.8523\n",
      "Epoch 9 | Batch 3101 | Loss: 0.99136 | Accuracy: 0.8519\n",
      "Epoch 9 | Batch 3201 | Loss: 1.02738 | Accuracy: 0.8523\n",
      "Epoch 9 | Batch 3301 | Loss: 1.04216 | Accuracy: 0.8530\n",
      "Epoch 9 | Batch 3401 | Loss: 0.96551 | Accuracy: 0.8531\n",
      "Epoch 9 | Batch 3501 | Loss: 1.05820 | Accuracy: 0.8527\n",
      "Epoch 9 | Batch 3601 | Loss: 0.89599 | Accuracy: 0.8519\n",
      "Epoch 9 | Batch 3701 | Loss: 1.15067 | Accuracy: 0.8521\n",
      "Epoch 9 | Batch 3801 | Loss: 1.06354 | Accuracy: 0.8534\n",
      "Epoch 9 | Batch 3901 | Loss: 0.97306 | Accuracy: 0.8532\n",
      "Epoch 9 | Batch 4001 | Loss: 1.02121 | Accuracy: 0.8541\n",
      "Epoch 9 | Batch 4101 | Loss: 0.98233 | Accuracy: 0.8530\n",
      "Epoch 9 | Batch 4201 | Loss: 0.80632 | Accuracy: 0.8538\n",
      "Epoch 9 | Batch 4301 | Loss: 0.91096 | Accuracy: 0.8531\n",
      "Epoch 9 | Batch 4401 | Loss: 0.87202 | Accuracy: 0.8542\n",
      "Epoch 9 | Batch 4501 | Loss: 0.85432 | Accuracy: 0.8541\n",
      "Epoch 9 | Batch 4601 | Loss: 0.91215 | Accuracy: 0.8535\n",
      "Batch shapes - X: torch.Size([32]), Pos: torch.Size([32, 2]), Neg: torch.Size([32, 2])\n",
      "Max indices - X: 3538, Pos: 3663, Neg: 3716\n",
      "Vocab size: 3718\n",
      "Epoch 10 | Batch 1 | Loss: 1.00987 | Accuracy: 0.8535\n",
      "Epoch 10 | Batch 101 | Loss: 0.99706 | Accuracy: 0.8535\n",
      "Epoch 10 | Batch 201 | Loss: 0.90570 | Accuracy: 0.8531\n",
      "Epoch 10 | Batch 301 | Loss: 0.99937 | Accuracy: 0.8542\n",
      "Epoch 10 | Batch 401 | Loss: 1.00814 | Accuracy: 0.8544\n",
      "Epoch 10 | Batch 501 | Loss: 0.87647 | Accuracy: 0.8542\n",
      "Epoch 10 | Batch 601 | Loss: 0.88767 | Accuracy: 0.8542\n",
      "Epoch 10 | Batch 701 | Loss: 0.82460 | Accuracy: 0.8550\n",
      "Epoch 10 | Batch 801 | Loss: 0.83577 | Accuracy: 0.8546\n",
      "Epoch 10 | Batch 901 | Loss: 1.03874 | Accuracy: 0.8553\n",
      "Epoch 10 | Batch 1001 | Loss: 0.96477 | Accuracy: 0.8547\n",
      "Epoch 10 | Batch 1101 | Loss: 0.83135 | Accuracy: 0.8546\n",
      "Epoch 10 | Batch 1201 | Loss: 0.97779 | Accuracy: 0.8552\n",
      "Epoch 10 | Batch 1301 | Loss: 0.90440 | Accuracy: 0.8556\n",
      "Epoch 10 | Batch 1401 | Loss: 0.79520 | Accuracy: 0.8546\n",
      "Epoch 10 | Batch 1501 | Loss: 0.88912 | Accuracy: 0.8548\n",
      "Epoch 10 | Batch 1601 | Loss: 0.86772 | Accuracy: 0.8547\n",
      "Epoch 10 | Batch 1701 | Loss: 0.99898 | Accuracy: 0.8548\n",
      "Epoch 10 | Batch 1801 | Loss: 0.88830 | Accuracy: 0.8555\n",
      "Epoch 10 | Batch 1901 | Loss: 0.83275 | Accuracy: 0.8559\n",
      "Epoch 10 | Batch 2001 | Loss: 1.03374 | Accuracy: 0.8556\n",
      "Epoch 10 | Batch 2101 | Loss: 0.83093 | Accuracy: 0.8554\n",
      "Epoch 10 | Batch 2201 | Loss: 0.93318 | Accuracy: 0.8556\n",
      "Epoch 10 | Batch 2301 | Loss: 1.02278 | Accuracy: 0.8565\n",
      "Epoch 10 | Batch 2401 | Loss: 1.03737 | Accuracy: 0.8561\n",
      "Epoch 10 | Batch 2501 | Loss: 0.91519 | Accuracy: 0.8564\n",
      "Epoch 10 | Batch 2601 | Loss: 1.04636 | Accuracy: 0.8560\n",
      "Epoch 10 | Batch 2701 | Loss: 0.92151 | Accuracy: 0.8569\n",
      "Epoch 10 | Batch 2801 | Loss: 0.99441 | Accuracy: 0.8555\n",
      "Epoch 10 | Batch 2901 | Loss: 1.05242 | Accuracy: 0.8567\n",
      "Epoch 10 | Batch 3001 | Loss: 0.72048 | Accuracy: 0.8574\n",
      "Epoch 10 | Batch 3101 | Loss: 0.83616 | Accuracy: 0.8568\n",
      "Epoch 10 | Batch 3201 | Loss: 0.94719 | Accuracy: 0.8573\n",
      "Epoch 10 | Batch 3301 | Loss: 0.79161 | Accuracy: 0.8575\n",
      "Epoch 10 | Batch 3401 | Loss: 0.97329 | Accuracy: 0.8571\n",
      "Epoch 10 | Batch 3501 | Loss: 0.98928 | Accuracy: 0.8570\n",
      "Epoch 10 | Batch 3601 | Loss: 1.07759 | Accuracy: 0.8578\n",
      "Epoch 10 | Batch 3701 | Loss: 0.86761 | Accuracy: 0.8576\n",
      "Epoch 10 | Batch 3801 | Loss: 0.95960 | Accuracy: 0.8579\n",
      "Epoch 10 | Batch 3901 | Loss: 0.98263 | Accuracy: 0.8573\n",
      "Epoch 10 | Batch 4001 | Loss: 0.99298 | Accuracy: 0.8580\n",
      "Epoch 10 | Batch 4101 | Loss: 0.99791 | Accuracy: 0.8581\n",
      "Epoch 10 | Batch 4201 | Loss: 0.81701 | Accuracy: 0.8573\n",
      "Epoch 10 | Batch 4301 | Loss: 0.92584 | Accuracy: 0.8579\n",
      "Epoch 10 | Batch 4401 | Loss: 0.95494 | Accuracy: 0.8582\n",
      "Epoch 10 | Batch 4501 | Loss: 0.86926 | Accuracy: 0.8581\n",
      "Epoch 10 | Batch 4601 | Loss: 0.96379 | Accuracy: 0.8582\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(RANDOM_SEED) # Set manual seed\n",
    "\n",
    "epochs = 10          # Set epochs\n",
    "batch_size = 32     # Set batch size\n",
    "\n",
    "lossi = []          # Track loss\n",
    "\n",
    "# Send data to the device\n",
    "# X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "# X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "\n",
    "# Create batches from dataset\n",
    "dataset = torch.utils.data.TensorDataset(X, Y)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, \n",
    "                                         batch_size=batch_size, \n",
    "                                         shuffle=True, \n",
    "                                         drop_last=True)\n",
    "\n",
    "# Loop through the data\n",
    "for epoch in range(epochs):\n",
    "    ### Training\n",
    "    total_loss = 0  # Track loss across batches\n",
    "    \n",
    "    for batch_idx, batch in enumerate(dataloader):\n",
    "        # Unpack batch\n",
    "        X_batch, positive_samples = batch\n",
    "        # Generate negative samples\n",
    "        negative_samples = torch.randint(0, vocab_size, (X_batch.size(0), context_len -1))\n",
    "\n",
    "        if batch_idx == 0:\n",
    "            print(f\"Batch shapes - X: {X_batch.shape}, Pos: {positive_samples.shape}, Neg: {negative_samples.shape}\")\n",
    "            print(f\"Max indices - X: {X_batch.max()}, Pos: {positive_samples.max()}, Neg: {negative_samples.max()}\")\n",
    "            print(f\"Vocab size: {vocab_size}\")\n",
    "\n",
    "        # 1. Forward pass (now returns loss directly)\n",
    "        loss = word2vec(X_batch, positive_samples, negative_samples)  # Model computes loss internally\n",
    "        \n",
    "        # 2. Zero the gradients\n",
    "        optimiser.zero_grad()\n",
    "        \n",
    "        # 3. Loss backwards\n",
    "        loss.backward()\n",
    "        \n",
    "        # 4. Step the optimiser\n",
    "        optimiser.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Track loss\n",
    "        lossi.append(loss.item())\n",
    "        \n",
    "        accuracy = evaluate_accuracy(word2vec, X, Y, vocab_size)\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f\"Epoch {epoch + 1} | Batch {batch_idx + 1} | Loss: {loss.item():.5f} | Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    # print(f\"Epoch {epoch + 1} | Loss: {loss.item():.5f} | Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    # print(f\"Epoch {epoch + 1} | Batch {batch_idx + 1} | Loss: {loss.item():.5f}\")\n",
    "    # Print out what's happening every 100 epochs\n",
    "    # if epoch % 100 == 0:\n",
    "    #     avg_loss = total_loss / len(dataloader)\n",
    "    #     print(f\"Epoch: {epoch} | Average Loss: {avg_loss:.5f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(lossi):\n",
    "    \"\"\"\n",
    "    Plot loss values using PyTorch operations.\n",
    "    \"\"\"\n",
    "    losses = torch.tensor(lossi)\n",
    "    window_size = max(len(losses) // 100, 1)\n",
    "    \n",
    "    # Reshape and mean\n",
    "    remainder = len(losses) % window_size\n",
    "    if remainder:\n",
    "        # Pad with the last value to make it evenly divisible\n",
    "        padding = window_size - remainder\n",
    "        losses = torch.cat([losses, losses[-1].repeat(padding)])\n",
    "    \n",
    "    averaged_losses = losses.view(-1, window_size).mean(1)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(averaged_losses)\n",
    "    plt.title('Training Loss Over Time')\n",
    "    plt.xlabel(f'Steps (averaged over {window_size} steps)')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss(lossi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(torch.tensor(lossi).view(-1, 1000).mean(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nearest_neighbors(word, stoi, embeddings, n=5):\n",
    "    \"\"\"\n",
    "    Find the top-n nearest neighbors of a word in the embedding space.\n",
    "    \n",
    "    Args:\n",
    "        word (str): The target word.\n",
    "        stoi (dict): Mapping from words to indices.\n",
    "        embeddings (torch.Tensor): Learned word embeddings (shape: V x d).\n",
    "        n (int): Number of nearest neighbors to retrieve.\n",
    "    \n",
    "    Returns:\n",
    "        List of tuples (neighbor_word, similarity_score).\n",
    "    \"\"\"\n",
    "    if word not in stoi:\n",
    "        return f\"'{word}' not in vocabulary.\"\n",
    "    \n",
    "    word_idx = stoi[word]\n",
    "    word_embedding = embeddings[word_idx].unsqueeze(0)  # Shape: 1 x d\n",
    "    \n",
    "    # Compute cosine similarity between the target embedding and all embeddings\n",
    "    similarities = cosine_similarity(word_embedding.detach().numpy(), embeddings.detach().numpy())\n",
    "    similarities = similarities[0]  # Flatten\n",
    "    \n",
    "    # Get top-n similar words (excluding the word itself)\n",
    "    nearest_indices = similarities.argsort()[-n-1:][::-1][1:]  # Exclude the word itself\n",
    "    nearest_words = [(list(stoi.keys())[idx], similarities[idx]) for idx in nearest_indices]\n",
    "    return nearest_words\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest neighbors of 'pollen':\n",
      "[('etude', 0.8218192), ('getting', 0.81094694), ('terminal', 0.788779), ('parakeet', 0.78516495), ('khat', 0.7850395)]\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "word = \"pollen\"\n",
    "nearest_neighbors = get_nearest_neighbors(word, word_to_idx, word2vec.embeddings.weight, n=5)\n",
    "print(f\"Nearest neighbors of '{word}':\")\n",
    "print(nearest_neighbors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random sample of words:\n",
      "['is', 'however', 'recall', 'usa', 'this', 'most', 'for', 'zero', 'rather', 'reserved', 'zero', 'two', 'population', 'source', 'function', 'legislature', 'sinless', 'anomaly', 'for', 'root']\n"
     ]
    }
   ],
   "source": [
    "# Using random\n",
    "# Make a copy so you don't shuffle the original list\n",
    "words_sample = processed_words.copy()\n",
    "random.shuffle(words_sample)\n",
    "\n",
    "# Print first 10 shuffled words\n",
    "print(\"Random sample of words:\")\n",
    "print(words_sample[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([63774, 20])"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.embeddings.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['anarchism',\n",
       " 'originated',\n",
       " 'a',\n",
       " 'a',\n",
       " 'term',\n",
       " 'of',\n",
       " 'abuse',\n",
       " 'first',\n",
       " 'used',\n",
       " 'against',\n",
       " 'early',\n",
       " 'working',\n",
       " 'class',\n",
       " 'radical']"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets build the regression model\n",
    "\n",
    "title = \"anarchism originated as a term of abuse first used against early working class radicals\"\n",
    "title_words, title_words_to_idx = preprocess_text(title,1)\n",
    "# title_idx = word_to_idx['anarchism']\n",
    "# title_words, \n",
    "title_words\n",
    "# word2vec.embeddings.weight[:10].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_indices(words: List[str], word_to_idx: dict) -> List[int]:\n",
    "    \"\"\"\n",
    "    Get indices for a list of words, with error handling for unknown words.\n",
    "    \n",
    "    Args:\n",
    "        words: List of words to look up\n",
    "        word_to_idx: Dictionary mapping words to indices\n",
    "    \n",
    "    Returns:\n",
    "        List of indices for the input words. Unknown words will be noted.\n",
    "    \"\"\"\n",
    "    indices = []\n",
    "    unknown_words = []\n",
    "    \n",
    "    for word in words:\n",
    "        if word in word_to_idx:\n",
    "            indices.append(word_to_idx[word])\n",
    "        else:\n",
    "            unknown_words.append(word)\n",
    "    \n",
    "    # Uncomment below to print unknown words\n",
    "    # if unknown_words:\n",
    "    #     print(f\"Warning: Words not in vocabulary: {unknown_words}\")\n",
    "    \n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([20]), torch.Size([14, 20]))"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get a tensor of indexes for tokenized words in input title\n",
    "title_idx = torch.tensor(get_word_indices(title_words, word_to_idx))\n",
    "\n",
    "word2vec.embeddings.weight[title_idx].shape, title_idx.shape\n",
    "\n",
    "avg_pooling = torch.mean(word2vec.embeddings.weight[title_idx], dim= 0)\n",
    "# avg_pooling.unsqueeze(dim=0).shape\n",
    "avg_pooling.shape, word2vec.embeddings.weight[title_idx].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the regression model\n",
    "\n",
    "class RegressionModel(nn.Module):\n",
    "    def __init__(self, emb_dims, hidden_dims):\n",
    "        super().__init__()\n",
    "        self.emb_dims = emb_dims\n",
    "        self.hidden_dims = hidden_dims\n",
    "\n",
    "        # Define layers\n",
    "        self.layer1 = nn.Linear(in_features=self.emb_dims, out_features=self.hidden_dims, bias= True)\n",
    "        self.layer2 = nn.Linear(in_features=self.hidden_dims, out_features=self.hidden_dims, bias= True)\n",
    "        self.out_layer = nn.Linear(in_features=self.hidden_dims, out_features= 1, bias= True)\n",
    "\n",
    "        # Define ReLU activation\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # Define loss function\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        h1 = self.relu(self.layer1(x))\n",
    "        h2 = self.relu(self.layer2(h1))\n",
    "        out = self.out_layer(h2)\n",
    "        return out\n",
    "    \n",
    "# Define loss function\n",
    "regression_loss_fn = nn.MSELoss()   \n",
    "\n",
    "# Set hidden dimensions\n",
    "hidden_dims = 20\n",
    "\n",
    "# Instantiate the model\n",
    "regression_model = RegressionModel(emb_dims= emb_dims, hidden_dims=hidden_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shapes - X: torch.Size([7567, 20]), y: torch.Size([7567, 1])\n",
      "Sample scores: tensor([ 8., 62.,  1.,  1.,  2.])\n"
     ]
    }
   ],
   "source": [
    "def get_title_embeddings(title: str, word2vec: nn.Module, word_to_idx: dict) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Process a single title and get its embedding representation.\n",
    "    \"\"\"\n",
    "    # Preprocess the title\n",
    "    title_words, _ = preprocess_text(title, min_freq=1)  # min_freq=1 since we're using existing vocabulary\n",
    "    \n",
    "    # Get indices for words that exist in our vocabulary\n",
    "    title_idx = get_word_indices(title_words, word_to_idx)\n",
    "    \n",
    "    # Uncomment below for printing when title not included \n",
    "    # if not title_idx:  # If no words from title are in vocabulary\n",
    "    #     print(f\"Warning: No words from title found in vocabulary: {title}\")\n",
    "    #     return None\n",
    "    \n",
    "    # Convert to tensor and get embeddings\n",
    "    title_tensor = torch.tensor(title_idx, dtype=torch.long)\n",
    "    word_embeddings = word2vec.embeddings.weight[title_tensor]\n",
    "    \n",
    "    # Average the word embeddings to get title embedding\n",
    "    title_embedding = torch.mean(word_embeddings, dim=0)\n",
    "    \n",
    "    return title_embedding\n",
    "\n",
    "# Process all titles in the dataframe\n",
    "def prepare_regression_dataset(df: pd.DataFrame, word2vec: nn.Module, word_to_idx: dict) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Prepare dataset for regression model.\n",
    "    \"\"\"\n",
    "    title_embeddings = []\n",
    "    scores = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        title_emb = get_title_embeddings(row['title'], word2vec, word_to_idx)\n",
    "        if title_emb is not None and not torch.isnan(title_emb).any():  # Only include if we got valid embeddings\n",
    "            title_embeddings.append(title_emb)\n",
    "            scores.append(row['score'])\n",
    "    \n",
    "    # Stack all embeddings and scores\n",
    "    X = torch.stack(title_embeddings)\n",
    "    Y = torch.tensor(scores, dtype=torch.float).reshape(-1, 1)\n",
    "    \n",
    "    return X, Y\n",
    "\n",
    "# Example usage:\n",
    "# First, ensure word2vec is trained on combined dataset\n",
    "# Then prepare regression dataset\n",
    "X_titles, Y_scores = prepare_regression_dataset(df, word2vec, word_to_idx)\n",
    "\n",
    "print(f\"Dataset shapes - X: {X_titles.shape}, y: {Y_scores.shape}\")\n",
    "print(f\"Sample scores: {Y_scores[:5].squeeze()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_titles[:10], Y_scores[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 3727\n",
      "Sample words: ['anarchism', 'originated', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against', 'early', 'working', 'class', 'radical', 'including', 'the', 'english', 'revolution', 'and', 'french', 'is']\n",
      "\n",
      "Problematic title: fedora packages versus upstream flatpaks\n",
      "Words in title: ['fedora', 'packages', 'versus', 'upstream', 'flatpaks']\n",
      "Words in vocabulary: []\n"
     ]
    }
   ],
   "source": [
    "# Print vocabulary statistics\n",
    "print(f\"Vocabulary size: {len(word_to_idx)}\")\n",
    "print(f\"Sample words: {list(word_to_idx.keys())[:20]}\")\n",
    "\n",
    "# Check a specific problematic title\n",
    "problem_idx = 4 # index where you found NaN\n",
    "title = df.iloc[problem_idx]['title']\n",
    "print(f\"\\nProblematic title: {title}\")\n",
    "print(f\"Words in title: {title.split()}\")\n",
    "print(f\"Words in vocabulary: {[word for word in title.split() if word in word_to_idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Dataset size: 7567\n",
      "Number of batches: 237\n",
      "Error occurred: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.\n",
      "Last batch shapes - X: torch.Size([32, 20]), y: torch.Size([32, 1])\n",
      "Last predictions shape: torch.Size([32, 1])\n"
     ]
    }
   ],
   "source": [
    "# Regression model training loop\n",
    "reg_epochs = 10\n",
    "batch_size = 32\n",
    "\n",
    "# Create DataLoader\n",
    "dataset = torch.utils.data.TensorDataset(X_titles, Y_scores)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, \n",
    "                                       batch_size=batch_size, \n",
    "                                       shuffle=True)\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = torch.optim.Adam(regression_model.parameters())\n",
    "losses = []\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(f\"Dataset size: {len(X_titles)}\")\n",
    "print(f\"Number of batches: {len(dataloader)}\")\n",
    "\n",
    "try:\n",
    "    for epoch in range(reg_epochs):\n",
    "        epoch_loss = 0\n",
    "        for batch_idx, (batch_X, batch_y) in enumerate(dataloader):\n",
    "            # Zero gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            predictions = regression_model(batch_X)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = regression_loss_fn(predictions, batch_y)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Update weights\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Track loss\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            if batch_idx % 10 == 0:\n",
    "                print(f\"Epoch {epoch+1}/{reg_epochs} | Batch {batch_idx}/{len(dataloader)} | \"\n",
    "                      f\"Loss: {loss.item():.4f}\")\n",
    "        \n",
    "        # Average loss for epoch\n",
    "        avg_loss = epoch_loss / len(dataloader)\n",
    "        losses.append(avg_loss)\n",
    "        print(f\"Epoch {epoch+1}/{reg_epochs} complete | Avg Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error occurred: {str(e)}\")\n",
    "    print(f\"Last batch shapes - X: {batch_X.shape}, y: {batch_y.shape}\")\n",
    "    print(f\"Last predictions shape: {predictions.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[315], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m predictions \u001b[38;5;241m=\u001b[39m regression_model(X_titles)\n\u001b[1;32m     13\u001b[0m loss \u001b[38;5;241m=\u001b[39m regression_loss_fn(predictions, Y_scores)\n\u001b[0;32m---> 14\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/autograd/graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "# Regression model training loop\n",
    "reg_epochs = 10\n",
    "\n",
    "optimizer = torch.optim.Adam(regression_model.parameters())\n",
    "\n",
    "# Create DataLoader\n",
    "dataset = torch.utils.data.TensorDataset(X_titles, Y_scores)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, \n",
    "                                       batch_size=batch_size, \n",
    "                                       shuffle=True)\n",
    "\n",
    "# NEED TO HANDLE INCOMING DATASET, \n",
    "# PROGRESS BAR\n",
    "\n",
    "for epoch in range(reg_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    predictions = regression_model(X_titles)\n",
    "    loss = regression_loss_fn(predictions, Y_scores)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
