{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# Setup device agnostic code\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "device\n",
    "\n",
    "# Setup random seed\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.0</td>\n",
       "      <td>nasa's 3d-printed rotating detonation rocket e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>62.0</td>\n",
       "      <td>heat pumps of the 1800s are becoming the techn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>why you should develop local-first web apps</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>tool to make twitter archive publishable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>fedora packages versus upstream flatpaks</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   score                                              title\n",
       "0    8.0  nasa's 3d-printed rotating detonation rocket e...\n",
       "1   62.0  heat pumps of the 1800s are becoming the techn...\n",
       "2    1.0        why you should develop local-first web apps\n",
       "3    1.0           tool to make twitter archive publishable\n",
       "4    2.0           fedora packages versus upstream flatpaks"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../HN Score, Title 10k.csv')\n",
    "df.title = df.title.str.lower()\n",
    "df.dropna(inplace= True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(71, 10)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = []\n",
    "num_lines = 10\n",
    "lines = df.title.tolist()[:num_lines]\n",
    "\n",
    "for i in df.title[:num_lines]:\n",
    "    for j in str(i).split():\n",
    "        if j not in words and j != \"nan\":\n",
    "            words.append(j)\n",
    "\n",
    "vocab_size = len(words)\n",
    "\n",
    "vocab_size, len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "itos = {num:word for num, word in zip(range(len(words)),words)}\n",
    "stoi = {word:num for num,word in itos.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8216"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_skipgram_pairs(input_lines, context_len):\n",
    "    \"\"\"\n",
    "    Generate Skip-Gram pairs (target, context) from a list of input lines.\n",
    "\n",
    "    Args:\n",
    "        input_lines (list of str): The input lines of text.\n",
    "        context_len (int): Context window size (number of words on each side).\n",
    "\n",
    "    Returns:\n",
    "        targets (list): List of target words.\n",
    "        context_words (list of lists): List of context word lists for each target.\n",
    "    \"\"\"\n",
    "    context_words = []\n",
    "    targets = []\n",
    "\n",
    "    for line in input_lines:\n",
    "        # Split the line into words (not letters)\n",
    "        words = line.split()\n",
    "        for i, target in enumerate(words):\n",
    "            # Define context window boundaries\n",
    "            start = max(0, i - context_len)\n",
    "            end = min(len(words), i + context_len + 1)\n",
    "\n",
    "            # Collect context words, excluding the target word itself\n",
    "            context = [words[j] for j in range(start, end) if j != i]\n",
    "            targets.append(target)\n",
    "            context_words.append(context)\n",
    "\n",
    "            # print(f\"Target: {target} ; Context: {context}\")\n",
    "\n",
    "    return targets, context_words\n",
    "\n",
    "context_len = 1\n",
    "\n",
    "targets, context_words = create_skipgram_pairs(lines, context_len)\n",
    "\n",
    "targets_len = len(targets)\n",
    "targets_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert targets to indices\n",
    "target_indices = [stoi[target] for target in targets if target in stoi]\n",
    "\n",
    "# Convert context words to indices\n",
    "context_indices = [[stoi[context] for context in contexts if context in stoi] for contexts in context_words]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8216]), torch.Size([8216, 3989]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert target indices to PyTorch tensor\n",
    "X = torch.tensor(target_indices, dtype=torch.long)\n",
    "\n",
    "# Create one-hot encoded targets (X)\n",
    "# X = torch.zeros((targets_len, vocab_size))\n",
    "# X.scatter_(1, target_indices.unsqueeze(1), 1)  # Scatter 1s into the appropriate indices\n",
    "# X = X.long()\n",
    "\n",
    "# Convert context indices to PyTorch tensor and one-hot encode (Y)\n",
    "Y = torch.zeros((targets_len, vocab_size))\n",
    "for i, context in enumerate(context_indices):\n",
    "    Y[i, context] = 1  # Set 1s for all indices in the context for each target\n",
    "\n",
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dims = 20\n",
    "\n",
    "class Word2Vec(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dims):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(num_embeddings= vocab_size, embedding_dim= emb_dims)\n",
    "        self.layer2 = nn.Linear(in_features = emb_dims, out_features = vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.out = self.layer2(self.emb(x))\n",
    "        return self.out\n",
    "\n",
    "word2vec = Word2Vec(vocab_size = vocab_size, emb_dims= emb_dims)\n",
    "\n",
    "# W1 = torch.randn(vocab_size, emb_dims)\n",
    "# W2 = torch.randn(vocab_size, emb_dims)\n",
    "\n",
    "# parameters = [W1, W2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup loss function\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Setup optimizer to optimize model's parameters\n",
    "optimiser = torch.optim.SGD(params= word2vec.parameters(), lr = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's calculuate the accuracy using accuracy from TorchMetrics\n",
    "# !pip -q install torchmetrics # Colab doesn't come with torchmetrics\n",
    "from torchmetrics import Accuracy\n",
    "\n",
    "## TODO: Uncomment this code to use the Accuracy function\n",
    "acc_fn = Accuracy(task=\"multiclass\", num_classes=vocab_size) # send accuracy function to device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Loss: 0.71978, Accuracy: 51.90%\n",
      "Epoch: 100 | Loss: 0.71629, Accuracy: 52.36%\n",
      "Epoch: 200 | Loss: 0.71284, Accuracy: 52.83%\n",
      "Epoch: 300 | Loss: 0.70941, Accuracy: 53.30%\n",
      "Epoch: 400 | Loss: 0.70600, Accuracy: 53.76%\n",
      "Epoch: 500 | Loss: 0.70261, Accuracy: 54.23%\n",
      "Epoch: 600 | Loss: 0.69924, Accuracy: 54.69%\n",
      "Epoch: 700 | Loss: 0.69590, Accuracy: 55.17%\n",
      "Epoch: 800 | Loss: 0.69258, Accuracy: 55.64%\n",
      "Epoch: 900 | Loss: 0.68928, Accuracy: 56.12%\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "# Setup epochs\n",
    "epochs = 1000\n",
    "\n",
    "# Send data to the device\n",
    "# X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "# X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "\n",
    "# Loop through the data\n",
    "for epoch in range(epochs):\n",
    "  ### Training\n",
    "\n",
    "  # 1. Forward pass (logits output)\n",
    "  y_logits = word2vec(X)\n",
    "  # Turn logits into prediction probabilities\n",
    "  y_pred_probs = torch.sigmoid(y_logits)\n",
    "\n",
    "  # Turn prediction probabilities into prediction labels\n",
    "  y_pred_labels = torch.round(y_pred_probs)\n",
    "\n",
    "  # 2. Calculaute the loss\n",
    "  loss = loss_fn(y_logits, Y) # loss = compare model raw outputs to desired model outputs\n",
    "\n",
    "  # Calculate the accuracy\n",
    "  acc = acc_fn(y_pred_labels, Y) # the accuracy function needs to compare pred labels (not logits) with actual labels\n",
    "\n",
    "  # 3. Zero the gradients\n",
    "  optimiser.zero_grad()\n",
    "\n",
    "  # 4. Loss backwards\n",
    "  loss.backward()\n",
    "\n",
    "  # 5. Step the optimiser\n",
    "  optimiser.step()\n",
    "\n",
    "  # Print out what's happening every 100 epochs\n",
    "  if epoch % 100 == 0:\n",
    "    print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Accuracy: {acc:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for p in parameters:\n",
    "#     p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs = 30000\n",
    "# lossi = []\n",
    "\n",
    "# for i in range(epochs):\n",
    "#     h = X @ W1\n",
    "#     logits = h @ W2.T\n",
    "\n",
    "#     # Compute the loss\n",
    "#     loss = F.binary_cross_entropy_with_logits(logits, Y)\n",
    "\n",
    "#     # backward pass\n",
    "#     for p in parameters:\n",
    "#         p.grad = None\n",
    "#     loss.backward()\n",
    "#     # Update\n",
    "#     lr = 0.1 if i <= 30000 else 0.01 if i <= 55000 else 0.001\n",
    "#     for p in parameters:\n",
    "#         p.data -= lr * p.grad\n",
    "\n",
    "#     # track stats\n",
    "#     if i % 1000 == 0: # print every once in a while\n",
    "#         print(f'{i:7d}/{epochs:7d}: {loss.item():.4f}')\n",
    "#     lossi.append(loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lossi' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[43mlossi\u001b[49m)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1000\u001b[39m)\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;241m1\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lossi' is not defined"
     ]
    }
   ],
   "source": [
    "# plt.plot(torch.tensor(lossi).view(-1, 1000).mean(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest neighbors of 'rocket':\n",
      "[('guardsman', 0.6874943), ('spreading', 0.68435574), ('solutions', 0.68200964), ('douglas', 0.66121185), ('cz', 0.61781037)]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def get_nearest_neighbors(word, stoi, embeddings, n=5):\n",
    "    \"\"\"\n",
    "    Find the top-n nearest neighbors of a word in the embedding space.\n",
    "    \n",
    "    Args:\n",
    "        word (str): The target word.\n",
    "        stoi (dict): Mapping from words to indices.\n",
    "        embeddings (torch.Tensor): Learned word embeddings (shape: V x d).\n",
    "        n (int): Number of nearest neighbors to retrieve.\n",
    "    \n",
    "    Returns:\n",
    "        List of tuples (neighbor_word, similarity_score).\n",
    "    \"\"\"\n",
    "    if word not in stoi:\n",
    "        return f\"'{word}' not in vocabulary.\"\n",
    "    \n",
    "    word_idx = stoi[word]\n",
    "    word_embedding = embeddings[word_idx].unsqueeze(0)  # Shape: 1 x d\n",
    "    \n",
    "    # Compute cosine similarity between the target embedding and all embeddings\n",
    "    similarities = cosine_similarity(word_embedding.detach().numpy(), embeddings.detach().numpy())\n",
    "    similarities = similarities[0]  # Flatten\n",
    "    \n",
    "    # Get top-n similar words (excluding the word itself)\n",
    "    nearest_indices = similarities.argsort()[-n-1:][::-1][1:]  # Exclude the word itself\n",
    "    nearest_words = [(list(stoi.keys())[idx], similarities[idx]) for idx in nearest_indices]\n",
    "    return nearest_words\n",
    "\n",
    "# Example usage\n",
    "word = \"rocket\"\n",
    "nearest_neighbors = get_nearest_neighbors(word, stoi, W1, n=5)\n",
    "print(f\"Nearest neighbors of '{word}':\")\n",
    "print(nearest_neighbors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"nasa's\",\n",
       " '3d-printed',\n",
       " 'rotating',\n",
       " 'detonation',\n",
       " 'rocket',\n",
       " 'engine',\n",
       " 'test',\n",
       " 'a',\n",
       " 'success',\n",
       " 'heat',\n",
       " 'pumps',\n",
       " 'of',\n",
       " 'the',\n",
       " '1800s',\n",
       " 'are',\n",
       " 'becoming',\n",
       " 'technology',\n",
       " 'future',\n",
       " 'why',\n",
       " 'you']"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
