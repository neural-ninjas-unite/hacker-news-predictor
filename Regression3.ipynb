{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "import wikipediaapi\n",
    "import re\n",
    "from typing import List, Dict\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Dict, List, Optional\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "import wikipediaapi\n",
    "import re\n",
    "from typing import List, Dict\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class HackerNewsDataCollector:\n",
    "    def __init__(self, db_url: str):\n",
    "        \"\"\"Initialize the HackerNews data collector with database connection.\"\"\"\n",
    "        self.db_url = db_url\n",
    "        self.engine = create_engine(db_url)\n",
    "    \n",
    "    def fetch_hn_data(self) -> pd.DataFrame:\n",
    "        \"\"\"Fetch Hacker News data from PostgreSQL database.\"\"\"\n",
    "        query = \"\"\"\n",
    "        SELECT \n",
    "            title,\n",
    "            score,\n",
    "            by as author,\n",
    "            descendants as num_comments,\n",
    "            text as comment_text\n",
    "        FROM stories\n",
    "        WHERE score > 0\n",
    "        \"\"\"\n",
    "        try:\n",
    "            df = pd.read_sql(query, self.engine)\n",
    "            logger.info(f\"Successfully fetched {len(df)} records from HackerNews\")\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error fetching HackerNews data: {str(e)}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WikipediaDataCollector:\n",
    "    def __init__(self, language: str = 'en'):\n",
    "        \"\"\"Initialize Wikipedia API client.\"\"\"\n",
    "        self.wiki = wikipediaapi.Wikipedia(language)\n",
    "        \n",
    "    def get_articles_by_category(self, category: str, max_articles: int = 1000) -> List[str]:\n",
    "        \"\"\"Fetch Wikipedia articles from a specific category.\"\"\"\n",
    "        category_page = self.wiki.page(f\"Category:{category}\")\n",
    "        articles = []\n",
    "        \n",
    "        def collect_articles(category_page, depth=0, max_depth=2):\n",
    "            if depth >= max_depth or len(articles) >= max_articles:\n",
    "                return\n",
    "            \n",
    "            for member in category_page.categorymembers.values():\n",
    "                if len(articles) >= max_articles:\n",
    "                    break\n",
    "                    \n",
    "                if member.ns == wikipediaapi.Namespace.MAIN:\n",
    "                    # Clean and add article text\n",
    "                    clean_text = self._clean_text(member.text)\n",
    "                    if clean_text:\n",
    "                        articles.append(clean_text)\n",
    "                        \n",
    "                elif member.ns == wikipediaapi.Namespace.CATEGORY and depth < max_depth:\n",
    "                    collect_articles(member, depth + 1, max_depth)\n",
    "        \n",
    "        collect_articles(category_page)\n",
    "        logger.info(f\"Collected {len(articles)} articles from Wikipedia category: {category}\")\n",
    "        return articles\n",
    "    \n",
    "    @staticmethod\n",
    "    def _clean_text(text: str) -> str:\n",
    "        \"\"\"Clean Wikipedia article text.\"\"\"\n",
    "        # Remove references, URLs, and special characters\n",
    "        text = re.sub(r'\\[\\d+\\]', '', text)  # Remove reference numbers\n",
    "        text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
    "        text = re.sub(r'\\s+', ' ', text)     # Normalize whitespace\n",
    "        return text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_data(db_url: str, wiki_categories: List[str]) -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"Collect both HackerNews and Wikipedia training data.\"\"\"\n",
    "    # Collect HackerNews data\n",
    "    hn_collector = HackerNewsDataCollector(db_url)\n",
    "    hn_data = hn_collector.fetch_hn_data()\n",
    "    \n",
    "    # Collect Wikipedia data\n",
    "    wiki_collector = WikipediaDataCollector()\n",
    "    wiki_texts = []\n",
    "    for category in wiki_categories:\n",
    "        wiki_texts.extend(wiki_collector.get_articles_by_category(category))\n",
    "    \n",
    "    wiki_data = pd.DataFrame({'text': wiki_texts})\n",
    "    \n",
    "    return {\n",
    "        'hacker_news': hn_data,\n",
    "        'wikipedia': wiki_data\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from typing import List, Tuple\n",
    "import logging\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class Word2VecDataset(Dataset):\n",
    "    def __init__(self, texts: List[str], window_size: int = 5, min_count: int = 5):\n",
    "        \"\"\"\n",
    "        Initialize Word2Vec dataset.\n",
    "        \n",
    "        Args:\n",
    "            texts: List of text documents\n",
    "            window_size: Context window size\n",
    "            min_count: Minimum word frequency\n",
    "        \"\"\"\n",
    "        self.window_size = window_size\n",
    "        \n",
    "        # Preprocess texts and build vocabulary\n",
    "        self.word_counts = Counter()\n",
    "        self.processed_texts = []\n",
    "        \n",
    "        for text in texts:\n",
    "            words = self._preprocess_text(text)\n",
    "            self.word_counts.update(words)\n",
    "            self.processed_texts.append(words)\n",
    "        \n",
    "        # Filter vocabulary by minimum count\n",
    "        self.vocab = {word: idx + 1 for idx, (word, count) in \n",
    "                     enumerate([item for item in self.word_counts.items() \n",
    "                              if item[1] >= min_count])}\n",
    "        self.vocab['<UNK>'] = 0\n",
    "        \n",
    "        self.idx_to_word = {idx: word for word, idx in self.vocab.items()}\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        \n",
    "        # Create training pairs\n",
    "        self.pairs = self._create_pairs()\n",
    "        \n",
    "    def _preprocess_text(self, text: str) -> List[str]:\n",
    "        \"\"\"Clean and tokenize text.\"\"\"\n",
    "        # Convert to lowercase and split into words\n",
    "        text = text.lower()\n",
    "        # Remove special characters and numbers\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "        return text.split()\n",
    "    \n",
    "    def _create_pairs(self) -> List[Tuple[int, int]]:\n",
    "        \"\"\"Create (target, context) pairs for training.\"\"\"\n",
    "        pairs = []\n",
    "        for words in self.processed_texts:\n",
    "            word_ids = [self.vocab.get(word, 0) for word in words]\n",
    "            \n",
    "            for i in range(len(word_ids)):\n",
    "                target = word_ids[i]\n",
    "                # Generate context words within window\n",
    "                for j in range(max(0, i - self.window_size), \n",
    "                             min(len(word_ids), i + self.window_size + 1)):\n",
    "                    if i != j:\n",
    "                        context = word_ids[j]\n",
    "                        pairs.append((target, context))\n",
    "        \n",
    "        return pairs\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.pairs)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        target, context = self.pairs[idx]\n",
    "        return torch.tensor(target), torch.tensor(context)\n",
    "\n",
    "class Word2Vec(nn.Module):\n",
    "    def __init__(self, vocab_size: int, embedding_dim: int):\n",
    "        \"\"\"\n",
    "        Initialize Word2Vec model.\n",
    "        \n",
    "        Args:\n",
    "            vocab_size: Size of vocabulary\n",
    "            embedding_dim: Dimension of word embeddings\n",
    "        \"\"\"\n",
    "        super(Word2Vec, self).__init__()\n",
    "        \n",
    "        self.target_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.context_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # Initialize embeddings\n",
    "        self.target_embeddings.weight.data.uniform_(-0.1, 0.1)\n",
    "        self.context_embeddings.weight.data.uniform_(-0.1, 0.1)\n",
    "    \n",
    "    def forward(self, target: torch.Tensor, context: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass of the model.\"\"\"\n",
    "        target_embeds = self.target_embeddings(target)\n",
    "        context_embeds = self.context_embeddings(context)\n",
    "        \n",
    "        # Compute dot product between target and context embeddings\n",
    "        output = torch.sum(target_embeds * context_embeds, dim=1)\n",
    "        return output\n",
    "    \n",
    "    def get_word_vector(self, word_idx: int) -> torch.Tensor:\n",
    "        \"\"\"Get the embedding vector for a word.\"\"\"\n",
    "        return self.target_embeddings.weight[word_idx].detach()\n",
    "\n",
    "def train_word2vec(texts: List[str], \n",
    "                  embedding_dim: int = 100,\n",
    "                  window_size: int = 5,\n",
    "                  min_count: int = 5,\n",
    "                  batch_size: int = 1024,\n",
    "                  num_epochs: int = 5,\n",
    "                  learning_rate: float = 0.025) -> Tuple[Word2Vec, Word2VecDataset]:\n",
    "    \"\"\"\n",
    "    Train Word2Vec model on input texts.\n",
    "    \n",
    "    Args:\n",
    "        texts: List of input texts\n",
    "        embedding_dim: Dimension of word embeddings\n",
    "        window_size: Context window size\n",
    "        min_count: Minimum word frequency\n",
    "        batch_size: Training batch size\n",
    "        num_epochs: Number of training epochs\n",
    "        learning_rate: Learning rate\n",
    "    \n",
    "    Returns:\n",
    "        Trained model and dataset\n",
    "    \"\"\"\n",
    "    # Create dataset\n",
    "    dataset = Word2VecDataset(texts, window_size, min_count)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Initialize model\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = Word2Vec(dataset.vocab_size, embedding_dim).to(device)\n",
    "    \n",
    "    # Initialize optimizer and loss\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    # Training loop\n",
    "    logger.info(\"Starting Word2Vec training...\")\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for batch_idx, (target, context) in enumerate(dataloader):\n",
    "            target = target.to(device)\n",
    "            context = context.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            output = model(target, context)\n",
    "            loss = criterion(output, torch.ones_like(output))\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        logger.info(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    return model, dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Dict, List, Optional\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class FeatureExtractor:\n",
    "    def __init__(self, word2vec_model, word2vec_dataset):\n",
    "        \"\"\"Initialize feature extractor with trained word2vec model.\"\"\"\n",
    "        self.word2vec_model = word2vec_model\n",
    "        self.word2vec_dataset = word2vec_dataset\n",
    "        self.numerical_scaler = StandardScaler()\n",
    "        \n",
    "    def extract_title_features(self, titles: List[str]) -> torch.Tensor:\n",
    "        \"\"\"Extract features from article titles using word2vec.\"\"\"\n",
    "        title_vectors = []\n",
    "        \n",
    "        for title in titles:\n",
    "            # Preprocess title\n",
    "            words = self.word2vec_dataset._preprocess_text(title)\n",
    "            # Get word indices\n",
    "            word_indices = [self.word2vec_dataset.vocab.get(word, 0) for word in words]\n",
    "            \n",
    "            if not word_indices:\n",
    "                # If no valid words, use zero vector\n",
    "                title_vectors.append(torch.zeros(self.word2vec_model.target_embeddings.weight.shape[1]))\n",
    "                continue\n",
    "            \n",
    "            # Get word vectors and average them\n",
    "            vectors = [self.word2vec_model.get_word_vector(idx) for idx in word_indices]\n",
    "            title_vector = torch.stack(vectors).mean(dim=0)\n",
    "            title_vectors.append(title_vector)\n",
    "        \n",
    "        return torch.stack(title_vectors)\n",
    "    \n",
    "    def extract_numerical_features(self, data: Dict[str, List]) -> np.ndarray:\n",
    "        \"\"\"Extract and normalize numerical features.\"\"\"\n",
    "        numerical_features = np.column_stack([\n",
    "            data['num_comments']\n",
    "        ])\n",
    "        \n",
    "        # Fit scaler if not already fit\n",
    "        if not hasattr(self.numerical_scaler, 'mean_'):\n",
    "            self.numerical_scaler.fit(numerical_features)\n",
    "        \n",
    "        return self.numerical_scaler.transform(numerical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LateFusionModel(nn.Module):\n",
    "    def __init__(self, text_embedding_dim: int, num_numerical_features: int):\n",
    "        \"\"\"\n",
    "        Initialize Late Fusion model.\n",
    "        \n",
    "        Args:\n",
    "            text_embedding_dim: Dimension of text embeddings from word2vec\n",
    "            num_numerical_features: Number of numerical features\n",
    "        \"\"\"\n",
    "        super(LateFusionModel, self).__init__()\n",
    "        \n",
    "        # Text feature processing\n",
    "        self.text_network = nn.Sequential(\n",
    "            nn.Linear(text_embedding_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 32)\n",
    "        )\n",
    "        \n",
    "        # Numerical feature processing\n",
    "        self.numerical_network = nn.Sequential(\n",
    "            nn.Linear(num_numerical_features, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(16, 8)\n",
    "        )\n",
    "        \n",
    "        # Fusion layer\n",
    "        fusion_input_dim = 32 + 8  # Combined dimensions from both networks\n",
    "        self.fusion_network = nn.Sequential(\n",
    "            nn.Linear(fusion_input_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, text_features: torch.Tensor, numerical_features: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "        \n",
    "        Args:\n",
    "            text_features: Tensor of text features from word2vec\n",
    "            numerical_features: Tensor of numerical features\n",
    "        \"\"\"\n",
    "        # Process text features\n",
    "        text_output = self.text_network(text_features)\n",
    "        \n",
    "        # Process numerical features\n",
    "        numerical_output = self.numerical_network(numerical_features)\n",
    "        \n",
    "        # Concatenate features\n",
    "        combined = torch.cat([text_output, numerical_output], dim=1)\n",
    "        \n",
    "        # Final prediction\n",
    "        output = self.fusion_network(combined)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_late_fusion(model: LateFusionModel,\n",
    "                     feature_extractor: FeatureExtractor,\n",
    "                     train_data: Dict,\n",
    "                     num_epochs: int = 10,\n",
    "                     batch_size: int = 32,\n",
    "                     learning_rate: float = 0.001) -> LateFusionModel:\n",
    "    \"\"\"\n",
    "    Train the Late Fusion model.\n",
    "    \n",
    "    Args:\n",
    "        model: Late Fusion model instance\n",
    "        feature_extractor: Feature extractor instance\n",
    "        train_data: Dictionary containing training data\n",
    "        num_epochs: Number of training epochs\n",
    "        batch_size: Training batch size\n",
    "        learning_rate: Learning rate\n",
    "    \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Extract features\n",
    "    title_features = feature_extractor.extract_title_features(train_data['title'])\n",
    "    numerical_features = feature_extractor.extract_numerical_features(train_data)\n",
    "    \n",
    "    # Convert to tensors\n",
    "    title_features = title_features.to(device)\n",
    "    numerical_features = torch.FloatTensor(numerical_features).to(device)\n",
    "    targets = torch.FloatTensor(train_data['score']).to(device)\n",
    "    \n",
    "    # Initialize optimizer and loss\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # Training loop\n",
    "    n_samples = len(train_data['title'])\n",
    "    n_batches = (n_samples + batch_size - 1) // batch_size\n",
    "    \n",
    "    logger.info(\"Starting Late Fusion model training...\")\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        \n",
    "        for i in range(n_batches):\n",
    "            start_idx = i * batch_size\n",
    "            end_idx = min((i + 1) * batch_size, n_samples)\n",
    "            \n",
    "            # Get batch\n",
    "            batch_title_features = title_features[start_idx:end_idx]\n",
    "            batch_numerical_features = numerical_features[start_idx:end_idx]\n",
    "            batch_targets = targets[start_idx:end_idx]\n",
    "            \n",
    "            # Forward pass\n",
    "            predictions = model(batch_title_features, batch_numerical_features)\n",
    "            loss = criterion(predictions.squeeze(), batch_targets)\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / n_batches\n",
    "        logger.info(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import torch\n",
    "from typing import List\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from data_collection import get_training_data\n",
    "from word2vec_model import train_word2vec\n",
    "from late_fusion_model import LateFusionModel, FeatureExtractor, train_late_fusion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Hacker News Predictor package.\n",
    "This package contains modules for predicting Hacker News article scores using a Late Fusion model.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
